"""
å°æ¬§é—´è° - æ¬§æ´²æµ·å…³ç›‘æ§å‘˜
è´Ÿè´£æ¯å¤©ç›‘æ§æ¬§æ´²æµ·å…³ç›¸å…³æ–°é—»ï¼Œå…³æ³¨ç¬¬ä¸‰å›½è¿›å£æ¬§æ´²çš„æ”¿ç­–å˜åŒ–
é‡ç‚¹å…³é”®è¯ï¼šåå€¾é”€ã€è¿›å£é…é¢ã€å…³ç¨è°ƒæ•´ã€æ¬§æ´²å·ç¨ã€æ¬§æ´²æ´—é»‘é’±ç­‰
"""
import json
import hashlib
import asyncio
from typing import Dict, Any, List, Optional
from datetime import datetime
from loguru import logger
import httpx

from app.agents.base import BaseAgent, AgentRegistry
from app.models.conversation import AgentType
from app.core.config import settings


class EUCustomsMonitorAgent(BaseAgent):
    """å°æ¬§é—´è° - æ¬§æ´²æµ·å…³ç›‘æ§å‘˜"""
    
    name = "å°æ¬§é—´è°"
    agent_type = AgentType.EU_CUSTOMS_MONITOR
    description = "æ¬§æ´²æµ·å…³ç›‘æ§å‘˜ - æ¯å¤©ç›‘æ§æ¬§æ´²æµ·å…³æ–°é—»ï¼Œå…³æ³¨åå€¾é”€ã€å…³ç¨è°ƒæ•´ã€è¿›å£æ”¿ç­–ç­‰"
    
    # ç›‘æ§å…³é”®è¯ï¼ˆä¸­æ–‡ï¼‰
    MONITOR_KEYWORDS_CN = [
        # æ ¸å¿ƒå…³é”®è¯
        "æ¬§ç›Ÿåå€¾é”€",
        "æ¬§æ´²è¿›å£é…é¢",
        "æ¬§ç›Ÿå…³ç¨è°ƒæ•´",
        "æ¬§æ´²å·ç¨",
        "æ¬§æ´²æ´—é»‘é’±",
        # æ”¿ç­–ç±»
        "æ¬§ç›Ÿæµ·å…³æ”¿ç­–",
        "æ¬§ç›Ÿè¿›å£æ–°è§„",
        "æ¬§æ´²æ¸…å…³æ”¿ç­–",
        "æ¬§ç›ŸVATæ–°è§„",
        "æ¬§æ´²å…³ç¨å£å’",
        # å›½å®¶/åœ°åŒº
        "å¾·å›½æµ·å…³",
        "æ³•å›½æµ·å…³",
        "è‹±å›½æµ·å…³",
        "è·å…°æµ·å…³",
        "æ„å¤§åˆ©æµ·å…³",
        # ç¬¬ä¸‰å›½ç›¸å…³
        "ä¸­å›½å•†å“ æ¬§ç›Ÿ",
        "ç¬¬ä¸‰å›½è¿›å£ æ¬§æ´²",
        "ä¸­æ¬§è´¸æ˜“",
        "å¯¹åå…³ç¨",
        # è¡Œä¸šå½±å“
        "æ¬§æ´²ç‰©æµæ”¿ç­–",
        "æ¬§ç›Ÿç”µå•†æ³•è§„",
        "è·¨å¢ƒç”µå•† æ¬§æ´²æ–°è§„",
    ]
    
    # ç›‘æ§å…³é”®è¯ï¼ˆè‹±æ–‡ï¼Œç”¨äºæœç´¢å›½é™…æ–°é—»ï¼‰
    MONITOR_KEYWORDS_EN = [
        "EU anti-dumping",
        "European customs policy",
        "EU import quota",
        "EU tariff changes",
        "European Commission trade",
        "EU customs regulation",
        "third country import EU",
    ]
    
    # æ–°é—»æ¥æºé…ç½®
    NEWS_SOURCES = {
        "eu_official": {
            "name": "æ¬§ç›Ÿå®˜æ–¹",
            "site_filter": "site:ec.europa.eu OR site:europa.eu",
            "keywords": MONITOR_KEYWORDS_EN,
            "weight": 5  # æƒé‡æœ€é«˜
        },
        "china_mofcom": {
            "name": "ä¸­å›½å•†åŠ¡éƒ¨",
            "site_filter": "site:mofcom.gov.cn",
            "keywords": ["æ¬§ç›Ÿ", "æ¬§æ´²", "åå€¾é”€", "å…³ç¨"],
            "weight": 4
        },
        "customs_news": {
            "name": "æµ·å…³æ–°é—»",
            "site_filter": "site:customs.gov.cn OR site:chinaports.com",
            "keywords": ["æ¬§æ´²", "æ¬§ç›Ÿ", "è¿›å£"],
            "weight": 3
        },
        "industry_media": {
            "name": "è¡Œä¸šåª’ä½“",
            "site_filter": "site:56ec.com OR site:wuliu.com.cn OR site:chinawuliu.com.cn",
            "keywords": ["æ¬§æ´²", "æ¬§ç›Ÿ", "æ¸…å…³", "å…³ç¨"],
            "weight": 3
        },
        "google_general": {
            "name": "ç»¼åˆæœç´¢",
            "site_filter": "",
            "keywords": MONITOR_KEYWORDS_CN[:10],  # ä½¿ç”¨å‰10ä¸ªæ ¸å¿ƒå…³é”®è¯
            "weight": 2
        }
    }
    
    # é‡è¦æ€§åˆ¤æ–­å…³é”®è¯ï¼ˆå‡ºç°è¿™äº›è¯çš„æ–°é—»æ›´é‡è¦ï¼‰
    IMPORTANCE_KEYWORDS = [
        "ç´§æ€¥", "é‡å¤§", "çªå‘", "ç«‹å³ç”Ÿæ•ˆ", "æ–°è§„å®æ–½",
        "åå€¾é”€ç¨", "æƒ©ç½šæ€§å…³ç¨", "ç¦æ­¢è¿›å£", "æš‚åœè¿›å£",
        "è°ƒæŸ¥", "åˆ¶è£", "å¤„ç½š", "ç½šæ¬¾", "æŸ¥è·", "èµ°ç§",
        "urgent", "breaking", "new regulation", "immediate effect"
    ]
    
    def _build_system_prompt(self) -> str:
        return """ä½ æ˜¯å°æ¬§é—´è°ï¼Œä¸€ä½ä¸“ä¸šçš„æ¬§æ´²æµ·å…³æƒ…æŠ¥åˆ†æå‘˜ã€‚

ä½ çš„å·¥ä½œæ˜¯åˆ†ææ¬§æ´²æµ·å…³ç›¸å…³æ–°é—»ï¼Œåˆ¤æ–­å…¶å¯¹ç‰©æµè¡Œä¸šçš„é‡è¦æ€§å’Œå½±å“ã€‚

åˆ†æç»´åº¦ï¼š
1. æ–°é—»ç±»å‹ï¼šæ”¿ç­–å˜åŒ–/åå€¾é”€æªæ–½/å…³ç¨è°ƒæ•´/æ‰§æ³•è¡ŒåŠ¨/è¡Œä¸šåŠ¨æ€
2. å½±å“èŒƒå›´ï¼šæ¶‰åŠå“ªäº›å›½å®¶ã€å“ªäº›å•†å“ç±»åˆ«
3. ç´§æ€¥ç¨‹åº¦ï¼šæ˜¯å¦éœ€è¦ç«‹å³å…³æ³¨å’Œé‡‡å–è¡ŒåŠ¨
4. å¯¹ç‰©æµä¸šåŠ¡çš„å½±å“ï¼šæ¸…å…³ã€è¿è´¹ã€æ—¶æ•ˆç­‰æ–¹é¢

è¾“å‡ºè¦æ±‚ï¼š
- æ‰€æœ‰å†…å®¹å¿…é¡»ä½¿ç”¨ä¸­æ–‡è¾“å‡º
- å¦‚æœåŸæ–‡æ˜¯è‹±æ–‡ï¼Œè¯·ç¿»è¯‘æˆä¸­æ–‡
- æä¾›ç®€æ´æ˜äº†çš„åˆ†ææ‘˜è¦
- ç»™å‡ºå…·ä½“çš„ä¸šåŠ¡å»ºè®®

è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š
{
    "is_important": true/false,
    "importance_score": 0-100,
    "news_type": "æ”¿ç­–å˜åŒ–/åå€¾é”€/å…³ç¨è°ƒæ•´/æ‰§æ³•è¡ŒåŠ¨/è¡Œä¸šåŠ¨æ€",
    "title_cn": "ä¸­æ–‡æ ‡é¢˜",
    "summary_cn": "ä¸­æ–‡æ‘˜è¦ï¼ˆ100å­—ä»¥å†…ï¼‰",
    "affected_countries": ["æ¶‰åŠå›½å®¶"],
    "affected_products": ["æ¶‰åŠå•†å“ç±»åˆ«"],
    "impact_analysis": "å¯¹ç‰©æµä¸šåŠ¡çš„å½±å“åˆ†æ",
    "business_suggestion": "ä¸šåŠ¡å»ºè®®",
    "urgency": "ç´§æ€¥/é‡è¦/ä¸€èˆ¬"
}
"""
    
    async def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†æ¬§æ´²æµ·å…³æ–°é—»ç›‘æ§ä»»åŠ¡
        
        Args:
            input_data: {
                "action": "monitor" | "search" | "analyze" | "get_stats",
                "keywords": ["è‡ªå®šä¹‰å…³é”®è¯"],  # å¯é€‰
                "sources": ["news_source_ids"],  # å¯é€‰
                "max_results": æœ€å¤§ç»“æœæ•°é‡  # å¯é€‰
            }
        """
        action = input_data.get("action", "monitor")
        
        # å¼€å§‹ä»»åŠ¡ä¼šè¯ï¼ˆå®æ—¶ç›´æ’­ï¼‰
        await self.start_task_session(action, f"æ¬§æ´²æµ·å…³æ–°é—»ç›‘æ§: {action}")
        
        try:
            if action == "monitor":
                # å®Œæ•´çš„ç›‘æ§æµç¨‹
                result = await self._full_monitor(input_data)
            elif action == "search":
                # ä»…æœç´¢ï¼Œä¸åˆ†æ
                result = await self._search_news(input_data)
            elif action == "analyze":
                # åˆ†æå•æ¡æ–°é—»
                result = await self._analyze_single_news(input_data)
            elif action == "get_stats":
                # è·å–ç»Ÿè®¡ä¿¡æ¯
                result = await self._get_monitor_stats()
            else:
                result = {"error": f"æœªçŸ¥æ“ä½œ: {action}"}
            
            await self.end_task_session(f"å®Œæˆ{action}ä»»åŠ¡")
            return result
        except Exception as e:
            await self.end_task_session(error_message=str(e))
            raise
    
    async def _full_monitor(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å®Œæ•´çš„ç›‘æ§æµç¨‹ï¼šæœç´¢ -> åˆ†æ -> å­˜å‚¨ -> é€šçŸ¥
        """
        self.log("ğŸ” å¼€å§‹æ¬§æ´²æµ·å…³æ–°é—»ç›‘æ§...")
        start_time = datetime.now()
        
        # æ£€æŸ¥APIé…ç½®
        api_key = getattr(settings, 'SERPER_API_KEY', None)
        if not api_key:
            self.log("Serper APIæœªé…ç½®ï¼Œæ— æ³•è¿›è¡Œæœç´¢", "error")
            await self.log_error("Serper APIæœªé…ç½®", "è¯·åœ¨ç³»ç»Ÿè®¾ç½®ä¸­é…ç½®APIå¯†é’¥")
            return {
                "error": "æœç´¢APIæœªé…ç½®",
                "message": "è¯·åœ¨ç³»ç»Ÿè®¾ç½®ä¸­é…ç½® SERPER_API_KEY ä»¥å¯ç”¨æ–°é—»ç›‘æ§åŠŸèƒ½"
            }
        
        results = {
            "monitor_time": datetime.now().isoformat(),
            "sources_searched": [],
            "news_found": [],
            "important_news": [],
            "total_news": 0,
            "important_count": 0,
            "notification_sent": False
        }
        
        try:
            from app.models.database import async_session_maker
            from sqlalchemy import text
            
            async with async_session_maker() as db:
                all_news = []
                
                # 1. ä»å„ä¸ªæ¥æºæœç´¢æ–°é—»
                for source_id, source_config in self.NEWS_SOURCES.items():
                    source_name = source_config["name"]
                    site_filter = source_config["site_filter"]
                    keywords = source_config.get("keywords", self.MONITOR_KEYWORDS_CN[:5])
                    
                    await self.log_live_step("search", f"æœç´¢{source_name}æ–°é—»", f"æ¥æº: {source_id}")
                    
                    for keyword in keywords[:3]:  # æ¯ä¸ªæ¥æºä½¿ç”¨å‰3ä¸ªå…³é”®è¯
                        try:
                            query = f"{keyword} {site_filter}".strip()
                            self.log(f"ğŸ” æœç´¢: {query}")
                            
                            search_results = await self._search_with_serper(query)
                            
                            if search_results:
                                results["sources_searched"].append(source_name)
                                for item in search_results[:5]:  # æ¯ä¸ªå…³é”®è¯å–å‰5æ¡
                                    url = item.get("url", "")
                                    if not url:
                                        continue
                                    
                                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                                    url_hash = hashlib.md5(url.encode()).hexdigest()
                                    existing = await db.execute(
                                        text("SELECT id FROM eu_customs_news WHERE url_hash = :hash"),
                                        {"hash": url_hash}
                                    )
                                    if existing.fetchone():
                                        continue
                                    
                                    item["source_id"] = source_id
                                    item["source_name"] = source_name
                                    item["keyword"] = keyword
                                    item["url_hash"] = url_hash
                                    all_news.append(item)
                            
                            # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                            await asyncio.sleep(0.3)
                            
                        except Exception as e:
                            self.log(f"æœç´¢å¤±è´¥ ({source_name}, {keyword}): {e}", "error")
                
                self.log(f"ğŸ“° è·å– {len(all_news)} æ¡æ–°URLå¾…åˆ†æ")
                await self.log_live_step("info", f"è·å– {len(all_news)} æ¡æ–°é—»", "å¼€å§‹AIåˆ†æ")
                
                # 2. AIåˆ†ææ¯æ¡æ–°é—»
                max_analyze = input_data.get("max_results", 30)
                
                for item in all_news[:max_analyze]:
                    try:
                        title = item.get("title", "")
                        content = item.get("content", item.get("snippet", ""))
                        url = item.get("url", "")
                        url_hash = item.get("url_hash", "")
                        
                        # è®°å½•æ­£åœ¨åˆ†æ
                        await self.log_fetch(url, title, {"source": item.get("source_name")})
                        
                        # AIåˆ†ææ–°é—»é‡è¦æ€§
                        await self.log_think("åˆ†ææ–°é—»é‡è¦æ€§å’Œå½±å“", title[:50])
                        analysis = await self._analyze_news_importance({
                            "title": title,
                            "content": content,
                            "url": url,
                            "source": item.get("source_name", "")
                        })
                        
                        is_important = analysis.get("is_important", False)
                        importance_score = analysis.get("importance_score", 0)
                        
                        # æ„å»ºæ–°é—»æ•°æ®
                        news_data = {
                            "title": title,
                            "title_cn": analysis.get("title_cn", title),
                            "content": content,
                            "summary_cn": analysis.get("summary_cn", ""),
                            "url": url,
                            "url_hash": url_hash,
                            "source_id": item.get("source_id", ""),
                            "source_name": item.get("source_name", ""),
                            "keyword": item.get("keyword", ""),
                            "news_type": analysis.get("news_type", "è¡Œä¸šåŠ¨æ€"),
                            "importance_score": importance_score,
                            "is_important": is_important,
                            "urgency": analysis.get("urgency", "ä¸€èˆ¬"),
                            "affected_countries": analysis.get("affected_countries", []),
                            "affected_products": analysis.get("affected_products", []),
                            "impact_analysis": analysis.get("impact_analysis", ""),
                            "business_suggestion": analysis.get("business_suggestion", ""),
                            "collected_at": datetime.now().isoformat()
                        }
                        
                        results["news_found"].append(news_data)
                        results["total_news"] += 1
                        
                        if is_important:
                            results["important_news"].append(news_data)
                            results["important_count"] += 1
                            await self.log_result(
                                f"ğŸš¨ å‘ç°é‡è¦æ–°é—»!",
                                f"{analysis.get('title_cn', title)[:50]}",
                                {"importance_score": importance_score, "urgency": analysis.get("urgency")}
                            )
                        
                        # ä¿å­˜åˆ°æ•°æ®åº“
                        await db.execute(
                            text("""
                                INSERT INTO eu_customs_news 
                                (title, title_cn, content, summary_cn, url, url_hash,
                                 source_id, source_name, keyword, news_type,
                                 importance_score, is_important, urgency,
                                 affected_countries, affected_products,
                                 impact_analysis, business_suggestion, created_at)
                                VALUES 
                                (:title, :title_cn, :content, :summary_cn, :url, :url_hash,
                                 :source_id, :source_name, :keyword, :news_type,
                                 :importance_score, :is_important, :urgency,
                                 :affected_countries, :affected_products,
                                 :impact_analysis, :business_suggestion, NOW())
                                ON CONFLICT (url_hash) DO NOTHING
                            """),
                            {
                                "title": news_data["title"],
                                "title_cn": news_data["title_cn"],
                                "content": news_data["content"][:2000],  # é™åˆ¶é•¿åº¦
                                "summary_cn": news_data["summary_cn"],
                                "url": news_data["url"],
                                "url_hash": news_data["url_hash"],
                                "source_id": news_data["source_id"],
                                "source_name": news_data["source_name"],
                                "keyword": news_data["keyword"],
                                "news_type": news_data["news_type"],
                                "importance_score": news_data["importance_score"],
                                "is_important": news_data["is_important"],
                                "urgency": news_data["urgency"],
                                "affected_countries": news_data["affected_countries"],
                                "affected_products": news_data["affected_products"],
                                "impact_analysis": news_data["impact_analysis"],
                                "business_suggestion": news_data["business_suggestion"]
                            }
                        )
                        
                    except Exception as e:
                        self.log(f"åˆ†ææ–°é—»å¤±è´¥: {e}", "error")
                
                await db.commit()
                
                # 3. æ›´æ–°AIå‘˜å·¥ä»»åŠ¡ç»Ÿè®¡
                await db.execute(
                    text("""
                        UPDATE ai_agents
                        SET tasks_completed_today = tasks_completed_today + 1,
                            total_tasks_completed = total_tasks_completed + 1,
                            last_active_at = NOW()
                        WHERE agent_type = 'eu_customs_monitor'
                    """)
                )
                await db.commit()
                
                # 4. å‘é€ä¼ä¸šå¾®ä¿¡é€šçŸ¥ï¼ˆå¦‚æœæœ‰é‡è¦æ–°é—»ï¼‰
                if results["important_news"]:
                    notification_result = await self._send_wechat_notification(results["important_news"])
                    results["notification_sent"] = notification_result.get("success", False)
                
        except Exception as e:
            self.log(f"ç›‘æ§ä»»åŠ¡å‡ºé”™: {e}", "error")
            results["error"] = str(e)
            await self.log_error(str(e), "ç›‘æ§ä»»åŠ¡å‡ºé”™")
        
        # å»é‡sources
        results["sources_searched"] = list(set(results["sources_searched"]))
        
        duration = (datetime.now() - start_time).total_seconds()
        results["duration_seconds"] = round(duration, 2)
        
        self.log(f"âœ… ç›‘æ§å®Œæˆï¼è€—æ—¶{duration:.1f}ç§’ï¼Œå‘ç° {results['total_news']} æ¡æ–°é—»ï¼Œ"
                 f"é‡è¦æ–°é—» {results['important_count']} æ¡")
        
        return results
    
    async def _search_with_serper(self, query: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨Serper APIæœç´¢æ–°é—»"""
        api_key = getattr(settings, 'SERPER_API_KEY', None)
        if not api_key:
            return []
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                # ä½¿ç”¨æ–°é—»æœç´¢API
                response = await client.post(
                    "https://google.serper.dev/news",
                    headers={
                        "X-API-KEY": api_key,
                        "Content-Type": "application/json"
                    },
                    json={
                        "q": query,
                        "gl": "cn",
                        "hl": "zh-cn",
                        "num": 10,
                        "tbs": "qdr:w"  # è¿‡å»ä¸€å‘¨çš„æ–°é—»
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    results = []
                    
                    for item in data.get("news", []):
                        results.append({
                            "title": item.get("title", ""),
                            "content": item.get("snippet", ""),
                            "url": item.get("link", ""),
                            "date": item.get("date", ""),
                            "source": item.get("source", "")
                        })
                    
                    return results
                else:
                    self.log(f"Serper APIè¿”å›é”™è¯¯: {response.status_code}", "error")
                    
        except Exception as e:
            self.log(f"Serperæœç´¢å¼‚å¸¸: {e}", "error")
        
        return []
    
    async def _analyze_news_importance(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨AIåˆ†ææ–°é—»é‡è¦æ€§"""
        title = input_data.get("title", "")
        content = input_data.get("content", "")
        url = input_data.get("url", "")
        source = input_data.get("source", "")
        
        if not title:
            return {"is_important": False, "reason": "æ ‡é¢˜ä¸ºç©º"}
        
        # å¿«é€Ÿåˆ¤æ–­ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«é‡è¦æ€§å…³é”®è¯
        combined_text = f"{title} {content}".lower()
        has_importance_keyword = any(kw.lower() in combined_text for kw in self.IMPORTANCE_KEYWORDS)
        
        # ä½¿ç”¨AIæ·±åº¦åˆ†æ
        prompt = f"""è¯·åˆ†æä»¥ä¸‹æ¬§æ´²æµ·å…³ç›¸å…³æ–°é—»çš„é‡è¦æ€§ï¼š

æ¥æºï¼š{source}
æ ‡é¢˜ï¼š{title}
å†…å®¹ï¼š{content[:500]}
URLï¼š{url}

è¯·ä»ä»¥ä¸‹è§’åº¦åˆ†æï¼š
1. è¿™æ˜¯ä»€ä¹ˆç±»å‹çš„æ–°é—»ï¼Ÿï¼ˆæ”¿ç­–å˜åŒ–/åå€¾é”€/å…³ç¨è°ƒæ•´/æ‰§æ³•è¡ŒåŠ¨/è¡Œä¸šåŠ¨æ€ï¼‰
2. å¯¹ç‰©æµè¡Œä¸šæœ‰ä»€ä¹ˆå½±å“ï¼Ÿ
3. éœ€è¦ç«‹å³å…³æ³¨å—ï¼Ÿ

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼Œæ‰€æœ‰å†…å®¹å¿…é¡»ä½¿ç”¨ä¸­æ–‡ã€‚"""
        
        try:
            response = await self.think([{"role": "user", "content": prompt}], temperature=0.3)
            
            # è§£æAIå›å¤
            json_start = response.find("{")
            json_end = response.rfind("}") + 1
            if json_start != -1 and json_end > json_start:
                result = json.loads(response[json_start:json_end])
                
                # å¦‚æœæœ‰é‡è¦æ€§å…³é”®è¯ï¼Œæå‡åˆ†æ•°
                if has_importance_keyword and result.get("importance_score", 0) < 70:
                    result["importance_score"] = min(result.get("importance_score", 50) + 20, 100)
                    if not result.get("is_important"):
                        result["is_important"] = result.get("importance_score", 0) >= 60
                
                return result
        except json.JSONDecodeError:
            self.log("AIåˆ†æç»“æœè§£æå¤±è´¥", "warning")
        except Exception as e:
            self.log(f"AIåˆ†æå¼‚å¸¸: {e}", "error")
        
        # å¦‚æœAIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åˆ¤æ–­
        return self._rule_based_importance(title, content, has_importance_keyword)
    
    def _rule_based_importance(self, title: str, content: str, has_importance_keyword: bool) -> Dict[str, Any]:
        """åŸºäºè§„åˆ™çš„é‡è¦æ€§åˆ¤æ–­ï¼ˆAIå¤±è´¥æ—¶çš„å¤‡é€‰ï¼‰"""
        importance_score = 30  # åŸºç¡€åˆ†
        
        # æ£€æŸ¥æ ‡é¢˜ä¸­çš„å…³é”®è¯
        title_keywords = ["åå€¾é”€", "å…³ç¨", "æ–°è§„", "æ”¿ç­–", "ç¦æ­¢", "å¤„ç½š", "è°ƒæŸ¥"]
        title_matches = sum(1 for kw in title_keywords if kw in title)
        importance_score += title_matches * 15
        
        if has_importance_keyword:
            importance_score += 20
        
        # æ£€æŸ¥å†…å®¹ä¸­çš„å…³é”®è¯
        content_keywords = ["ç”Ÿæ•ˆ", "å®æ–½", "é€šçŸ¥", "å…¬å‘Š", "å†³å®š"]
        content_matches = sum(1 for kw in content_keywords if kw in content)
        importance_score += content_matches * 10
        
        importance_score = min(importance_score, 100)
        
        return {
            "is_important": importance_score >= 60,
            "importance_score": importance_score,
            "news_type": "è¡Œä¸šåŠ¨æ€",
            "title_cn": title,
            "summary_cn": content[:100] if content else "",
            "affected_countries": [],
            "affected_products": [],
            "impact_analysis": "éœ€è¦è¿›ä¸€æ­¥åˆ†æ",
            "business_suggestion": "å»ºè®®å…³æ³¨åç»­å‘å±•",
            "urgency": "é‡è¦" if importance_score >= 70 else "ä¸€èˆ¬"
        }
    
    async def _send_wechat_notification(self, important_news: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å‘é€ä¼ä¸šå¾®ä¿¡é€šçŸ¥"""
        try:
            from app.services.notification import notification_service
            
            # æ„å»ºé€šçŸ¥å†…å®¹
            today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
            
            content = f"""ğŸ”” ã€æ¬§æ´²æµ·å…³æƒ…æŠ¥æ—¥æŠ¥ã€‘{today}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š ä»Šæ—¥å‘ç° {len(important_news)} æ¡é‡è¦æ–°é—»ï¼š

"""
            
            for i, news in enumerate(important_news[:5], 1):  # æœ€å¤šæ˜¾ç¤º5æ¡
                urgency_emoji = "ğŸš¨" if news.get("urgency") == "ç´§æ€¥" else "âš ï¸" if news.get("urgency") == "é‡è¦" else "ğŸ“Œ"
                content += f"""{urgency_emoji} {i}. {news.get('title_cn', news.get('title', ''))[:50]}
   ç±»å‹ï¼š{news.get('news_type', 'æœªçŸ¥')} | é‡è¦åº¦ï¼š{news.get('importance_score', 0)}åˆ†
   æ‘˜è¦ï¼š{news.get('summary_cn', '')[:80]}...
   å»ºè®®ï¼š{news.get('business_suggestion', 'æš‚æ— ')[:50]}

"""
            
            if len(important_news) > 5:
                content += f"... è¿˜æœ‰ {len(important_news) - 5} æ¡é‡è¦æ–°é—»ï¼Œè¯·ç™»å½•ç³»ç»ŸæŸ¥çœ‹\n\n"
            
            content += """â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ç”±å°æ¬§é—´è°è‡ªåŠ¨ç›‘æ§ | ç‰©æµè·å®¢AI"""
            
            # å‘é€é€šçŸ¥
            await notification_service.send_to_boss(
                title=f"ğŸ”” æ¬§æ´²æµ·å…³æƒ…æŠ¥æ—¥æŠ¥ {today}",
                content=content
            )
            
            self.log("âœ… ä¼ä¸šå¾®ä¿¡é€šçŸ¥å·²å‘é€")
            return {"success": True}
            
        except Exception as e:
            self.log(f"å‘é€ä¼ä¸šå¾®ä¿¡é€šçŸ¥å¤±è´¥: {e}", "error")
            return {"success": False, "error": str(e)}
    
    async def _search_news(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ä»…æœç´¢æ–°é—»ï¼Œä¸åšåˆ†æ"""
        keywords = input_data.get("keywords", self.MONITOR_KEYWORDS_CN[:5])
        sources = input_data.get("sources", list(self.NEWS_SOURCES.keys()))
        
        all_results = []
        
        for source_id in sources:
            if source_id not in self.NEWS_SOURCES:
                continue
            
            source_config = self.NEWS_SOURCES[source_id]
            site_filter = source_config["site_filter"]
            
            for keyword in keywords[:3]:
                query = f"{keyword} {site_filter}".strip()
                results = await self._search_with_serper(query)
                
                for item in results:
                    item["source_id"] = source_id
                    item["source_name"] = source_config["name"]
                    item["keyword"] = keyword
                    all_results.append(item)
                
                await asyncio.sleep(0.3)
        
        return {
            "search_time": datetime.now().isoformat(),
            "results": all_results,
            "count": len(all_results)
        }
    
    async def _analyze_single_news(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå•æ¡æ–°é—»"""
        return await self._analyze_news_importance(input_data)
    
    async def _get_monitor_stats(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§ç»Ÿè®¡ä¿¡æ¯"""
        try:
            from app.models.database import async_session_maker
            from sqlalchemy import text
            
            async with async_session_maker() as db:
                # ä»Šæ—¥ç»Ÿè®¡
                today_result = await db.execute(
                    text("""
                        SELECT 
                            COUNT(*) as total,
                            COUNT(*) FILTER (WHERE is_important = true) as important,
                            AVG(importance_score) as avg_score
                        FROM eu_customs_news
                        WHERE DATE(created_at) = CURRENT_DATE
                    """)
                )
                today = today_result.fetchone()
                
                # æœ¬å‘¨ç»Ÿè®¡
                week_result = await db.execute(
                    text("""
                        SELECT 
                            COUNT(*) as total,
                            COUNT(*) FILTER (WHERE is_important = true) as important
                        FROM eu_customs_news
                        WHERE created_at >= CURRENT_DATE - INTERVAL '7 days'
                    """)
                )
                week = week_result.fetchone()
                
                # æŒ‰ç±»å‹ç»Ÿè®¡
                type_result = await db.execute(
                    text("""
                        SELECT news_type, COUNT(*) as count
                        FROM eu_customs_news
                        WHERE created_at >= CURRENT_DATE - INTERVAL '7 days'
                        GROUP BY news_type
                        ORDER BY count DESC
                    """)
                )
                by_type = {row[0]: row[1] for row in type_result.fetchall()}
                
                # æœ€è¿‘çš„é‡è¦æ–°é—»
                recent_result = await db.execute(
                    text("""
                        SELECT title_cn, news_type, importance_score, urgency, created_at
                        FROM eu_customs_news
                        WHERE is_important = true
                        ORDER BY created_at DESC
                        LIMIT 5
                    """)
                )
                recent_important = [
                    {
                        "title": row[0],
                        "type": row[1],
                        "score": row[2],
                        "urgency": row[3],
                        "time": row[4].isoformat() if row[4] else None
                    }
                    for row in recent_result.fetchall()
                ]
                
                return {
                    "today": {
                        "total": today[0] if today else 0,
                        "important": today[1] if today else 0,
                        "avg_score": round(today[2], 1) if today and today[2] else 0
                    },
                    "this_week": {
                        "total": week[0] if week else 0,
                        "important": week[1] if week else 0
                    },
                    "by_type": by_type,
                    "recent_important": recent_important
                }
                
        except Exception as e:
            self.log(f"è·å–ç»Ÿè®¡å¤±è´¥: {e}", "error")
            return {"error": str(e)}


# åˆ›å»ºå•ä¾‹å¹¶æ³¨å†Œ
eu_customs_monitor_agent = EUCustomsMonitorAgent()
AgentRegistry.register(eu_customs_monitor_agent)
