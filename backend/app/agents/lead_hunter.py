"""
å°çŒ - çº¿ç´¢çŒæ‰‹ (24å°æ—¶æ™ºèƒ½ç‰ˆ)
è´Ÿè´£è‡ªåŠ¨åœ¨äº’è”ç½‘ä¸Šæœç´¢æ½œåœ¨å®¢æˆ·çº¿ç´¢
æ”¯æŒï¼š
- 24å°æ—¶ä¸é—´æ–­æœç´¢
- æ™ºèƒ½å…³é”®è¯è½®æ¢
- æœç´¢æ•ˆæœè¿½è¸ª
- URLå»é‡
- è‡ªåŠ¨ä¼˜åŒ–æœç´¢ç­–ç•¥
"""
from typing import Dict, Any, List, Optional
import json
import re
import asyncio
import httpx
import hashlib
from datetime import datetime, timedelta
from loguru import logger

from app.agents.base import BaseAgent, AgentRegistry
from app.models.conversation import AgentType
from app.core.config import settings


class LeadHunterAgent(BaseAgent):
    """å°çŒ - çº¿ç´¢çŒæ‰‹ (24å°æ—¶æ™ºèƒ½ç‰ˆ)"""
    
    name = "å°çŒ"
    agent_type = AgentType.LEAD_HUNTER
    description = "çº¿ç´¢çŒæ‰‹ - 24å°æ—¶è‡ªåŠ¨æœç´¢äº’è”ç½‘ä¸Šçš„æ½œåœ¨å®¢æˆ·çº¿ç´¢"
    
    # å¤‡ç”¨æœç´¢å…³é”®è¯ï¼ˆæ•°æ®åº“å…³é”®è¯ä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰- åªæœç´¢æ¬§æ´²ç›¸å…³
    FALLBACK_KEYWORDS = [
        "æ¬§æ´²è´§ä»£", "æ¬§æ´²ç‰©æµæ¨è", "æ¬§æ´²ç‰©æµæŠ¥ä»·",
        "æ¬§æ´²æ¸…å…³", "æ¬§æ´²æ´¾é€", "æ¬§æ´²åˆ°é—¨",
        "å¾·å›½ç‰©æµ", "æ³•å›½ç‰©æµ", "è‹±å›½ç‰©æµ",
        "å‘è´§åˆ°æ¬§æ´²", "æ¬§æ´²FBA", "æ¬§æ´²åŒæ¸…",
        "å¾·å›½FBA", "è‹±å›½FBA", "æ¬§æ´²å¡æ´¾"
    ]
    
    # æœç´¢å¹³å°é…ç½®
    PLATFORMS = {
        "weibo": {"site": "site:weibo.com", "weight": 3},
        "zhihu": {"site": "site:zhihu.com", "weight": 3},
        "tieba": {"site": "site:tieba.baidu.com", "weight": 2},
        "douyin": {"site": "site:douyin.com", "weight": 2},
        "xiaohongshu": {"site": "site:xiaohongshu.com", "weight": 2},
        "google": {"site": "", "weight": 4}  # å…¨ç½‘æœç´¢
    }
    
    # çº¿ç´¢è´¨é‡åˆ¤æ–­å…³é”®è¯
    HIGH_INTENT_KEYWORDS = [
        "æ€¥", "é©¬ä¸Š", "å°½å¿«", "æŠ¥ä»·", "ä»·æ ¼", "å¤šå°‘é’±",
        "ç«‹å³", "ä»Šå¤©", "æ˜å¤©", "è¿™å‘¨", "æƒ³å‘", "è¦å‘",
        "æ±‚æ¨è", "æœ‰æ²¡æœ‰", "è°çŸ¥é“", "å“ªå®¶å¥½",
        "urgent", "asap", "quote", "price", "how much"
    ]
    
    # å¹¿å‘Šè¿‡æ»¤å…³é”®è¯
    AD_FILTER_KEYWORDS = [
        "åŠ ç›Ÿ", "æ‹›å•†", "ä»£ç†", "å…è´¹è¯•ç”¨", "é™æ—¶ä¼˜æƒ ",
        "æ¬¢è¿å’¨è¯¢", "ä¸“ä¸šç‰©æµ", "æˆ‘å¸", "æˆ‘ä»¬å…¬å¸",
        "è”ç³»ç”µè¯", "ç‚¹å‡»å’¨è¯¢", "åœ¨çº¿å®¢æœ", "æ‹›ä»£ç†",
        "è¯šæ‹›", "ç«çƒ­æ‹›å•†"
    ]
    
    def _build_system_prompt(self) -> str:
        return """ä½ æ˜¯å°çŒï¼Œä¸€ä½ä¸“ä¸šçš„çº¿ç´¢çŒæ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æäº’è”ç½‘ä¸Šçš„å†…å®¹ï¼Œåˆ¤æ–­æ˜¯å¦æ˜¯æ½œåœ¨çš„ç‰©æµå®¢æˆ·çº¿ç´¢ã€‚

åˆ†ææ—¶è¯·è€ƒè™‘ï¼š
1. æ˜¯å¦æœ‰ç‰©æµ/è´§ä»£éœ€æ±‚ï¼ˆæ’é™¤ç‰©æµå…¬å¸çš„å¹¿å‘Šå’Œæ¨å¹¿ï¼‰
2. éœ€æ±‚çš„ç´§è¿«ç¨‹åº¦
3. æ˜¯å¦æ˜¯çœŸå®çš„å®¢æˆ·éœ€æ±‚ï¼ˆä¸æ˜¯ç‰©æµå…¬å¸å‘çš„ï¼‰
4. æ½œåœ¨ä»·å€¼å¤§å°

åˆ¤æ–­è§„åˆ™ï¼š
- å¦‚æœå†…å®¹æ˜¯ç‰©æµå…¬å¸çš„å¹¿å‘Šã€æ¨å¹¿ã€æ‹›å•†ï¼Œè¿”å› is_lead: false
- å¦‚æœå†…å®¹æ˜¯ä¸ªäººæˆ–ä¼ä¸šåœ¨å¯»æ‰¾ç‰©æµæœåŠ¡ï¼Œè¿”å› is_lead: true
- å¦‚æœå†…å®¹åŒ…å«å…·ä½“çš„å‘è´§éœ€æ±‚ï¼ˆå¦‚ç›®çš„åœ°ã€è´§ç‰©ç±»å‹ã€é‡é‡ï¼‰ï¼Œæé«˜æ„å‘ç­‰çº§

è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š
{
    "is_lead": true/false,
    "confidence": 0-100,
    "intent_level": "high/medium/low",
    "lead_type": "ä¸ªäºº/ä¼ä¸š/ç”µå•†å–å®¶/å¤–è´¸å…¬å¸",
    "needs": ["æµ·è¿", "ç©ºè¿", "æ¸…å…³", "FBA"],
    "contact_info": {
        "name": "",
        "phone": "",
        "email": "",
        "wechat": "",
        "company": ""
    },
    "summary": "ç®€çŸ­æè¿°è¿™ä¸ªçº¿ç´¢",
    "follow_up_suggestion": "è·Ÿè¿›å»ºè®®"
}
"""
    
    async def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†çº¿ç´¢æœç´¢ä»»åŠ¡
        
        Args:
            input_data: {
                "action": "search" | "analyze" | "hunt" | "smart_hunt",
                "source": "æœç´¢æ¥æº",
                "content": "è¦åˆ†æçš„å†…å®¹",
                "keywords": ["è‡ªå®šä¹‰å…³é”®è¯"],
                "max_keywords": æœ€å¤§å…³é”®è¯æ•°é‡,
                "max_results": æœ€å¤§ç»“æœæ•°é‡
            }
        """
        action = input_data.get("action", "smart_hunt")
        
        if action == "search":
            return await self._search_leads(input_data)
        elif action == "analyze":
            return await self._analyze_content(input_data)
        elif action == "hunt":
            return await self._full_hunt(input_data)
        elif action == "smart_hunt":
            return await self._smart_hunt(input_data)
        elif action == "get_stats":
            return await self._get_hunt_stats()
        else:
            return {"error": f"æœªçŸ¥æ“ä½œ: {action}"}
    
    async def _smart_hunt(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ™ºèƒ½çº¿ç´¢ç‹©çŒ - 24å°æ—¶è‡ªåŠ¨è¿è¡Œç‰ˆæœ¬
        - è‡ªåŠ¨ä»æ•°æ®åº“è·å–å¾…æœç´¢å…³é”®è¯
        - æ™ºèƒ½é€‰æ‹©æœç´¢å¹³å°
        - è‡ªåŠ¨å»é‡å’Œè®°å½•
        - è¿½è¸ªæœç´¢æ•ˆæœ
        """
        self.log("ğŸ¯ å¼€å§‹æ™ºèƒ½çº¿ç´¢ç‹©çŒä»»åŠ¡...")
        start_time = datetime.now()
        
        # æ£€æŸ¥APIé…ç½®
        api_key = getattr(settings, 'SERPER_API_KEY', None)
        if not api_key:
            self.log("Serper APIæœªé…ç½®ï¼Œæ— æ³•è¿›è¡Œæœç´¢", "error")
            return {
                "error": "æœç´¢APIæœªé…ç½®",
                "message": "è¯·åœ¨ç³»ç»Ÿè®¾ç½®ä¸­é…ç½® SERPER_API_KEY ä»¥å¯ç”¨çº¿ç´¢æœç´¢åŠŸèƒ½",
                "hunt_time": datetime.now().isoformat(),
                "sources_searched": [],
                "leads_found": [],
                "total_leads": 0
            }
        
        results = {
            "hunt_time": datetime.now().isoformat(),
            "hunt_mode": "smart_24h",
            "sources_searched": [],
            "leads_found": [],
            "total_leads": 0,
            "high_intent_leads": 0,
            "keywords_used": [],
            "search_queries": [],
            "new_urls": 0,
            "duplicate_urls": 0,
            "stats": {}
        }
        
        try:
            from app.models.database import async_session_maker
            from sqlalchemy import text
            
            async with async_session_maker() as db:
                # 1. è·å–å¾…æœç´¢çš„å…³é”®è¯ï¼ˆä¼˜å…ˆçº§é«˜ã€æ•ˆæœå¥½ã€å†·å´æ—¶é—´å·²è¿‡ï¼‰
                max_keywords = input_data.get("max_keywords", 5)
                
                keyword_result = await db.execute(
                    text("""
                        SELECT id, keyword, keyword_type, platform, priority, success_rate
                        FROM lead_hunt_keywords
                        WHERE is_active = true
                        AND (next_search_after IS NULL OR next_search_after <= NOW())
                        ORDER BY 
                            priority DESC,
                            success_rate DESC,
                            last_searched_at ASC NULLS FIRST
                        LIMIT :limit
                    """),
                    {"limit": max_keywords}
                )
                keywords_data = keyword_result.fetchall()
                
                # å¦‚æœæ•°æ®åº“æ²¡æœ‰å…³é”®è¯ï¼Œä½¿ç”¨å¤‡ç”¨å…³é”®è¯
                if not keywords_data:
                    self.log("æ•°æ®åº“æ— å¯ç”¨å…³é”®è¯ï¼Œä½¿ç”¨å¤‡ç”¨å…³é”®è¯")
                    keywords_to_use = self.FALLBACK_KEYWORDS[:max_keywords]
                    keywords_data = [(None, kw, 'fallback', None, 5, 0) for kw in keywords_to_use]
                
                self.log(f"æœ¬æ¬¡å°†ä½¿ç”¨ {len(keywords_data)} ä¸ªå…³é”®è¯æœç´¢")
                
                all_raw_results = []
                
                # 2. å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œæœç´¢
                for kw_data in keywords_data:
                    kw_id, keyword, kw_type, kw_platform, priority, success_rate = kw_data
                    results["keywords_used"].append(keyword)
                    
                    # ç¡®å®šæœç´¢å¹³å°
                    if kw_platform:
                        platforms_to_search = [(kw_platform, self.PLATFORMS.get(kw_platform, {}).get("site", ""))]
                    else:
                        # æ ¹æ®å½“å‰æ—¶é—´æ™ºèƒ½é€‰æ‹©å¹³å°
                        platforms_to_search = self._select_platforms_by_time()
                    
                    keyword_leads = 0
                    keyword_high_intent = 0
                    
                    for platform_name, site_filter in platforms_to_search:
                        try:
                            query = f"{keyword} {site_filter}".strip()
                            self.log(f"ğŸ” æœç´¢: {query}")
                            results["search_queries"].append(query)
                            
                            search_results = await self._search_with_serper(query)
                            
                            if search_results:
                                results["sources_searched"].append(platform_name)
                                
                                for item in search_results:
                                    url = item.get("url", "")
                                    if not url:
                                        continue
                                    
                                    # æ£€æŸ¥URLæ˜¯å¦å·²æœç´¢è¿‡
                                    url_hash = hashlib.md5(url.encode()).hexdigest()
                                    
                                    existing_url = await db.execute(
                                        text("""
                                            SELECT id, is_lead FROM lead_hunt_searched_urls
                                            WHERE url_hash = :hash
                                        """),
                                        {"hash": url_hash}
                                    )
                                    existing = existing_url.fetchone()
                                    
                                    if existing:
                                        results["duplicate_urls"] += 1
                                        continue
                                    
                                    results["new_urls"] += 1
                                    
                                    item["platform"] = platform_name
                                    item["keyword"] = keyword
                                    item["keyword_id"] = kw_id
                                    item["url_hash"] = url_hash
                                    all_raw_results.append(item)
                            
                            # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                            await asyncio.sleep(0.5)
                            
                        except Exception as e:
                            self.log(f"æœç´¢å¤±è´¥ ({platform_name}, {keyword}): {e}", "error")
                    
                    # æ›´æ–°å…³é”®è¯ç»Ÿè®¡ï¼ˆåœ¨åˆ†æå®Œæ‰€æœ‰ç»“æœåæ›´æ–°ï¼‰
                
                self.log(f"ğŸ“Š è·å– {len(all_raw_results)} æ¡æ–°URLå¾…åˆ†æ")
                
                # 3. åˆ†ææ¯ä¸ªæœç´¢ç»“æœ
                max_results = input_data.get("max_results", 30)
                keyword_stats = {}  # è®°å½•æ¯ä¸ªå…³é”®è¯çš„æ•ˆæœ
                
                for item in all_raw_results[:max_results]:
                    try:
                        content = f"{item.get('title', '')} {item.get('content', '')}"
                        url = item.get("url", "")
                        url_hash = item.get("url_hash", "")
                        keyword = item.get("keyword", "")
                        keyword_id = item.get("keyword_id")
                        platform = item.get("platform", "google")
                        
                        # å¿«é€Ÿè¿‡æ»¤
                        if self._quick_filter(content):
                            # è®°å½•ä¸ºéçº¿ç´¢URL
                            await db.execute(
                                text("""
                                    INSERT INTO lead_hunt_searched_urls 
                                    (url_hash, url, source_keyword, platform, is_lead)
                                    VALUES (:hash, :url, :keyword, :platform, false)
                                    ON CONFLICT (url_hash) DO NOTHING
                                """),
                                {"hash": url_hash, "url": url, "keyword": keyword, "platform": platform}
                            )
                            continue
                        
                        # AIæ·±åº¦åˆ†æ
                        analysis = await self._analyze_content({
                            "content": content,
                            "source": platform,
                            "url": url
                        })
                        
                        is_lead = analysis.get("is_lead", False)
                        intent_level = analysis.get("intent_level", "low")
                        is_high_intent = intent_level == "high"
                        
                        # åˆå§‹åŒ–å…³é”®è¯ç»Ÿè®¡
                        if keyword not in keyword_stats:
                            keyword_stats[keyword] = {"id": keyword_id, "leads": 0, "high_intent": 0}
                        
                        if is_lead:
                            # æå–è”ç³»æ–¹å¼
                            contact_info = analysis.get("contact_info", {})
                            extracted_contact = self._extract_contact_info(content)
                            for key, value in extracted_contact.items():
                                if value and not contact_info.get(key):
                                    contact_info[key] = value
                            
                            lead_data = {
                                "title": item.get("title", ""),
                                "content": content,
                                "url": url,
                                "source": platform,
                                "keyword": keyword,
                                "found_at": datetime.now().isoformat(),
                                "is_lead": True,
                                "confidence": analysis.get("confidence", 50),
                                "intent_level": intent_level,
                                "lead_type": analysis.get("lead_type", ""),
                                "needs": analysis.get("needs", []),
                                "contact_info": contact_info,
                                "summary": analysis.get("summary", ""),
                                "follow_up_suggestion": analysis.get("follow_up_suggestion", "")
                            }
                            
                            results["leads_found"].append(lead_data)
                            results["total_leads"] += 1
                            keyword_stats[keyword]["leads"] += 1
                            
                            if is_high_intent:
                                results["high_intent_leads"] += 1
                                keyword_stats[keyword]["high_intent"] += 1
                            
                            # ä¿å­˜çº¿ç´¢åˆ°æ•°æ®åº“
                            lead_insert = await db.execute(
                                text("""
                                    INSERT INTO leads 
                                    (source, source_url, source_content, content, 
                                     ai_confidence, intent_level, ai_summary, ai_suggestion,
                                     needs, status, created_at)
                                    VALUES (:source, :url, :raw_content, :content, 
                                            :confidence, :level, :summary, :suggestion,
                                            :needs, 'new', NOW())
                                    ON CONFLICT (source_url) DO NOTHING
                                    RETURNING id
                                """),
                                {
                                    "source": platform,
                                    "url": url,
                                    "raw_content": content,
                                    "content": json.dumps(lead_data, ensure_ascii=False),
                                    "confidence": analysis.get("confidence", 50) / 100.0,
                                    "level": {"high": "high", "medium": "medium", "low": "low"}.get(intent_level, "unknown"),
                                    "summary": analysis.get("summary", ""),
                                    "suggestion": analysis.get("follow_up_suggestion", ""),
                                    "needs": analysis.get("needs", [])
                                }
                            )
                            lead_row = lead_insert.fetchone()
                            lead_id = lead_row[0] if lead_row else None
                            
                            # è®°å½•å·²æœç´¢URLï¼ˆæ ‡è®°ä¸ºçº¿ç´¢ï¼‰
                            await db.execute(
                                text("""
                                    INSERT INTO lead_hunt_searched_urls 
                                    (url_hash, url, source_keyword, platform, is_lead, lead_id)
                                    VALUES (:hash, :url, :keyword, :platform, true, :lead_id)
                                    ON CONFLICT (url_hash) DO NOTHING
                                """),
                                {"hash": url_hash, "url": url, "keyword": keyword, 
                                 "platform": platform, "lead_id": lead_id}
                            )
                        else:
                            # è®°å½•ä¸ºéçº¿ç´¢URL
                            await db.execute(
                                text("""
                                    INSERT INTO lead_hunt_searched_urls 
                                    (url_hash, url, source_keyword, platform, is_lead)
                                    VALUES (:hash, :url, :keyword, :platform, false)
                                    ON CONFLICT (url_hash) DO NOTHING
                                """),
                                {"hash": url_hash, "url": url, "keyword": keyword, "platform": platform}
                            )
                            
                    except Exception as e:
                        self.log(f"åˆ†æå†…å®¹å¤±è´¥: {e}", "error")
                
                # 4. æ›´æ–°å…³é”®è¯æ•ˆæœç»Ÿè®¡
                for keyword, stats in keyword_stats.items():
                    if stats["id"]:
                        await db.execute(
                            text("""
                                SELECT update_keyword_stats(:kw_id, :leads, :high_intent)
                            """),
                            {"kw_id": stats["id"], "leads": stats["leads"], "high_intent": stats["high_intent"]}
                        )
                
                # 5. è®°å½•æœç´¢å†å²
                duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
                
                await db.execute(
                    text("""
                        INSERT INTO lead_hunt_history 
                        (keyword, search_query, results_count, leads_found, high_intent_leads, duration_ms)
                        VALUES (:keyword, :query, :results, :leads, :high_intent, :duration)
                    """),
                    {
                        "keyword": ", ".join(results["keywords_used"]),
                        "query": ", ".join(results["search_queries"][:10]),  # åªè®°å½•å‰10ä¸ª
                        "results": len(all_raw_results),
                        "leads": results["total_leads"],
                        "high_intent": results["high_intent_leads"],
                        "duration": duration_ms
                    }
                )
                
                # 6. æ›´æ–°æ¯æ—¥ç»Ÿè®¡
                today = datetime.now().date()
                await db.execute(
                    text("""
                        INSERT INTO lead_hunt_stats 
                        (stat_date, total_searches, total_results, total_leads, high_intent_leads, unique_urls)
                        VALUES (:date, 1, :results, :leads, :high_intent, :urls)
                        ON CONFLICT (stat_date) DO UPDATE SET
                            total_searches = lead_hunt_stats.total_searches + 1,
                            total_results = lead_hunt_stats.total_results + :results,
                            total_leads = lead_hunt_stats.total_leads + :leads,
                            high_intent_leads = lead_hunt_stats.high_intent_leads + :high_intent,
                            unique_urls = lead_hunt_stats.unique_urls + :urls,
                            updated_at = NOW()
                    """),
                    {
                        "date": today,
                        "results": len(all_raw_results),
                        "leads": results["total_leads"],
                        "high_intent": results["high_intent_leads"],
                        "urls": results["new_urls"]
                    }
                )
                
                # 7. æ›´æ–°å°çŒçš„ä»»åŠ¡ç»Ÿè®¡
                await db.execute(
                    text("""
                        UPDATE ai_agents
                        SET tasks_completed_today = tasks_completed_today + 1,
                            tasks_completed_total = tasks_completed_total + 1,
                            last_active_at = NOW(),
                            updated_at = NOW()
                        WHERE agent_type = 'lead_hunter'
                    """)
                )
                
                await db.commit()
        
        except Exception as e:
            self.log(f"æ™ºèƒ½ç‹©çŒå‡ºé”™: {e}", "error")
            results["error"] = str(e)
        
        # å»é‡sources_searched
        results["sources_searched"] = list(set(results["sources_searched"]))
        
        duration = (datetime.now() - start_time).total_seconds()
        results["stats"] = {
            "duration_seconds": round(duration, 2),
            "keywords_count": len(results["keywords_used"]),
            "queries_count": len(results["search_queries"]),
            "new_urls_analyzed": results["new_urls"],
            "duplicate_urls_skipped": results["duplicate_urls"]
        }
        
        self.log(f"âœ… æ™ºèƒ½ç‹©çŒå®Œæˆï¼è€—æ—¶{duration:.1f}ç§’ï¼Œæ–°URL {results['new_urls']} æ¡ï¼Œ"
                 f"å‘ç°çº¿ç´¢ {results['total_leads']} æ¡ï¼Œé«˜æ„å‘ {results['high_intent_leads']} æ¡")
        
        return results
    
    def _select_platforms_by_time(self) -> List[tuple]:
        """
        æ ¹æ®å½“å‰æ—¶é—´æ™ºèƒ½é€‰æ‹©æœç´¢å¹³å°
        ä¸åŒæ—¶é—´æ®µç”¨æˆ·æ´»è·ƒçš„å¹³å°ä¸åŒ
        """
        current_hour = datetime.now().hour
        
        # æ·±å¤œ/å‡Œæ™¨ (0-6ç‚¹) - æœç´¢é‡è¾ƒå°‘ï¼Œä¸»è¦æœGoogle
        if 0 <= current_hour < 6:
            return [
                ("google", ""),
                ("zhihu", self.PLATFORMS["zhihu"]["site"])
            ]
        # æ—©ä¸Š (6-9ç‚¹) - å¾®åšæ´»è·ƒ
        elif 6 <= current_hour < 9:
            return [
                ("weibo", self.PLATFORMS["weibo"]["site"]),
                ("google", "")
            ]
        # ä¸Šåˆå·¥ä½œæ—¶é—´ (9-12ç‚¹) - å…¨é¢æœç´¢
        elif 9 <= current_hour < 12:
            return [
                ("google", ""),
                ("zhihu", self.PLATFORMS["zhihu"]["site"]),
                ("weibo", self.PLATFORMS["weibo"]["site"])
            ]
        # åˆä¼‘æ—¶é—´ (12-14ç‚¹) - ç¤¾äº¤å¹³å°æ´»è·ƒ
        elif 12 <= current_hour < 14:
            return [
                ("weibo", self.PLATFORMS["weibo"]["site"]),
                ("xiaohongshu", self.PLATFORMS["xiaohongshu"]["site"]),
                ("google", "")
            ]
        # ä¸‹åˆå·¥ä½œæ—¶é—´ (14-18ç‚¹) - å…¨é¢æœç´¢
        elif 14 <= current_hour < 18:
            return [
                ("google", ""),
                ("zhihu", self.PLATFORMS["zhihu"]["site"]),
                ("tieba", self.PLATFORMS["tieba"]["site"])
            ]
        # æ™šé—´ (18-22ç‚¹) - ç¤¾äº¤å¹³å°æœ€æ´»è·ƒ
        elif 18 <= current_hour < 22:
            return [
                ("weibo", self.PLATFORMS["weibo"]["site"]),
                ("zhihu", self.PLATFORMS["zhihu"]["site"]),
                ("douyin", self.PLATFORMS["douyin"]["site"]),
                ("google", "")
            ]
        # æ·±å¤œ (22-24ç‚¹) - çŸ¥ä¹å¤œçŒ«å­æ´»è·ƒ
        else:
            return [
                ("zhihu", self.PLATFORMS["zhihu"]["site"]),
                ("google", "")
            ]
    
    async def _get_hunt_stats(self) -> Dict[str, Any]:
        """
        è·å–æœç´¢ç»Ÿè®¡æ•°æ®
        """
        try:
            from app.models.database import async_session_maker
            from sqlalchemy import text
            
            async with async_session_maker() as db:
                # ä»Šæ—¥ç»Ÿè®¡
                today_result = await db.execute(
                    text("""
                        SELECT total_searches, total_results, total_leads, 
                               high_intent_leads, unique_urls
                        FROM lead_hunt_stats
                        WHERE stat_date = CURRENT_DATE
                    """)
                )
                today = today_result.fetchone()
                
                # æœ¬å‘¨ç»Ÿè®¡
                week_result = await db.execute(
                    text("""
                        SELECT SUM(total_searches), SUM(total_results), 
                               SUM(total_leads), SUM(high_intent_leads)
                        FROM lead_hunt_stats
                        WHERE stat_date >= CURRENT_DATE - INTERVAL '7 days'
                    """)
                )
                week = week_result.fetchone()
                
                # æœ€ä½³å…³é”®è¯
                best_kw_result = await db.execute(
                    text("""
                        SELECT keyword, leads_found, success_rate
                        FROM lead_hunt_keywords
                        WHERE is_active = true AND search_count > 0
                        ORDER BY success_rate DESC
                        LIMIT 5
                    """)
                )
                best_keywords = best_kw_result.fetchall()
                
                return {
                    "today": {
                        "searches": today[0] if today else 0,
                        "results": today[1] if today else 0,
                        "leads": today[2] if today else 0,
                        "high_intent": today[3] if today else 0,
                        "unique_urls": today[4] if today else 0
                    },
                    "this_week": {
                        "searches": week[0] if week else 0,
                        "results": week[1] if week else 0,
                        "leads": week[2] if week else 0,
                        "high_intent": week[3] if week else 0
                    },
                    "best_keywords": [
                        {"keyword": kw[0], "leads": kw[1], "success_rate": round(kw[2] * 100, 1)}
                        for kw in best_keywords
                    ]
                }
        except Exception as e:
            self.log(f"è·å–ç»Ÿè®¡å¤±è´¥: {e}", "error")
            return {"error": str(e)}
    
    async def _full_hunt(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å®Œæ•´çš„çº¿ç´¢ç‹©çŒæµç¨‹ - ä½¿ç”¨Serper APIæœç´¢
        (ä¿ç•™åŸæœ‰æ–¹æ³•ä»¥ä¿æŒå…¼å®¹æ€§)
        """
        self.log("å¼€å§‹çº¿ç´¢ç‹©çŒä»»åŠ¡...")
        
        # æ£€æŸ¥APIé…ç½®
        api_key = getattr(settings, 'SERPER_API_KEY', None)
        if not api_key:
            self.log("Serper APIæœªé…ç½®ï¼Œæ— æ³•è¿›è¡Œæœç´¢", "error")
            return {
                "error": "æœç´¢APIæœªé…ç½®",
                "message": "è¯·åœ¨ç³»ç»Ÿè®¾ç½®ä¸­é…ç½® SERPER_API_KEY ä»¥å¯ç”¨çº¿ç´¢æœç´¢åŠŸèƒ½",
                "hunt_time": datetime.now().isoformat(),
                "sources_searched": [],
                "leads_found": [],
                "total_leads": 0
            }
        
        results = {
            "hunt_time": datetime.now().isoformat(),
            "sources_searched": [],
            "leads_found": [],
            "total_leads": 0,
            "high_intent_leads": 0,
            "search_queries": []
        }
        
        # è·å–è‡ªå®šä¹‰å…³é”®è¯æˆ–ä½¿ç”¨é»˜è®¤å…³é”®è¯
        keywords = input_data.get("keywords", self.FALLBACK_KEYWORDS[:6])
        
        # å®šä¹‰æœç´¢å¹³å°å’Œå¯¹åº”çš„siteé™å®š
        platforms = [
            ("weibo", "site:weibo.com"),
            ("zhihu", "site:zhihu.com"),
            ("tieba", "site:tieba.baidu.com"),
            ("google", "")  # å…¨ç½‘æœç´¢
        ]
        
        all_raw_results = []
        
        # å¯¹æ¯ä¸ªå…³é”®è¯å’Œå¹³å°ç»„åˆè¿›è¡Œæœç´¢
        for keyword in keywords[:3]:  # é™åˆ¶æœç´¢æ¬¡æ•°ï¼Œæ§åˆ¶APIè°ƒç”¨
            for platform_name, site_filter in platforms:
                try:
                    query = f"{keyword} {site_filter}".strip()
                    self.log(f"æœç´¢: {query}")
                    results["search_queries"].append(query)
                    
                    search_results = await self._search_with_serper(query)
                    
                    if search_results:
                        results["sources_searched"].append(platform_name)
                        for item in search_results:
                            item["platform"] = platform_name
                            item["keyword"] = keyword
                            all_raw_results.append(item)
                    
                    # é¿å…è¯·æ±‚è¿‡å¿«
                    await asyncio.sleep(0.5)
                    
                except Exception as e:
                    self.log(f"æœç´¢å¤±è´¥ ({platform_name}, {keyword}): {e}", "error")
        
        # å»é‡ï¼ˆæ ¹æ®URLï¼‰
        seen_urls = set()
        unique_results = []
        for item in all_raw_results:
            url = item.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(item)
        
        self.log(f"å…±è·å– {len(unique_results)} æ¡å»é‡åçš„æœç´¢ç»“æœ")
        
        # åˆ†ææ¯ä¸ªæœç´¢ç»“æœ
        for item in unique_results[:20]:  # é™åˆ¶åˆ†ææ•°é‡
            try:
                content = f"{item.get('title', '')} {item.get('content', '')}"
                
                # å¿«é€Ÿè¿‡æ»¤
                if self._quick_filter(content):
                    continue
                
                # AIæ·±åº¦åˆ†æ
                analysis = await self._analyze_content({
                    "content": content,
                    "source": item.get("platform", "google"),
                    "url": item.get("url", "")
                })
                
                if analysis.get("is_lead"):
                    # æå–è”ç³»æ–¹å¼
                    contact_info = analysis.get("contact_info", {})
                    extracted_contact = self._extract_contact_info(content)
                    # åˆå¹¶è”ç³»ä¿¡æ¯
                    for key, value in extracted_contact.items():
                        if value and not contact_info.get(key):
                            contact_info[key] = value
                    
                    lead_data = {
                        "title": item.get("title", ""),
                        "content": content,
                        "url": item.get("url", ""),
                        "source": item.get("platform", "google"),
                        "keyword": item.get("keyword", ""),
                        "found_at": datetime.now().isoformat(),
                        "is_lead": True,
                        "confidence": analysis.get("confidence", 50),
                        "intent_level": analysis.get("intent_level", "medium"),
                        "lead_type": analysis.get("lead_type", ""),
                        "needs": analysis.get("needs", []),
                        "contact_info": contact_info,
                        "summary": analysis.get("summary", ""),
                        "follow_up_suggestion": analysis.get("follow_up_suggestion", "")
                    }
                    
                    results["leads_found"].append(lead_data)
                    results["total_leads"] += 1
                    
                    if analysis.get("intent_level") == "high":
                        results["high_intent_leads"] += 1
                        
            except Exception as e:
                self.log(f"åˆ†æå†…å®¹å¤±è´¥: {e}", "error")
        
        # å»é‡sources_searched
        results["sources_searched"] = list(set(results["sources_searched"]))
        
        self.log(f"çº¿ç´¢ç‹©çŒå®Œæˆï¼æ‰¾åˆ° {results['total_leads']} æ¡çº¿ç´¢ï¼Œå…¶ä¸­é«˜æ„å‘ {results['high_intent_leads']} æ¡")
        
        return results
    
    async def _search_with_serper(self, query: str) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨Serper APIæœç´¢
        """
        api_key = getattr(settings, 'SERPER_API_KEY', None)
        if not api_key:
            return []
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    "https://google.serper.dev/search",
                    headers={
                        "X-API-KEY": api_key,
                        "Content-Type": "application/json"
                    },
                    json={
                        "q": query,
                        "gl": "cn",
                        "hl": "zh-cn",
                        "num": 10
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    results = []
                    
                    for item in data.get("organic", []):
                        results.append({
                            "title": item.get("title", ""),
                            "content": item.get("snippet", ""),
                            "url": item.get("link", ""),
                            "position": item.get("position", 0)
                        })
                    
                    return results
                else:
                    self.log(f"Serper APIè¿”å›é”™è¯¯: {response.status_code}", "error")
                    
        except Exception as e:
            self.log(f"Serperæœç´¢å¼‚å¸¸: {e}", "error")
        
        return []
    
    # å…¬å¼€æ–¹æ³•ä¾›å¤–éƒ¨è°ƒç”¨
    async def search_with_serper(self, query: str) -> List[Dict[str, Any]]:
        """å…¬å¼€çš„Serperæœç´¢æ–¹æ³•"""
        return await self._search_with_serper(query)
    
    async def _analyze_content(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä½¿ç”¨AIåˆ†æå†…å®¹æ˜¯å¦æ˜¯æ½œåœ¨å®¢æˆ·
        """
        content = input_data.get("content", "")
        source = input_data.get("source", "unknown")
        url = input_data.get("url", "")
        
        if not content:
            return {"is_lead": False, "reason": "å†…å®¹ä¸ºç©º"}
        
        # å¿«é€Ÿè§„åˆ™åˆ¤æ–­
        # æ£€æŸ¥æ˜¯å¦åŒ…å«é«˜æ„å‘å…³é”®è¯
        has_high_intent = any(kw in content for kw in self.HIGH_INTENT_KEYWORDS)
        
        # ç”¨AIæ·±åº¦åˆ†æ
        prompt = f"""è¯·åˆ†æä»¥ä¸‹å†…å®¹æ˜¯å¦æ˜¯æ½œåœ¨çš„ç‰©æµå®¢æˆ·çº¿ç´¢ï¼š

æ¥æºå¹³å°ï¼š{source}
URLï¼š{url}
å†…å®¹ï¼š{content}

æ³¨æ„ï¼š
1. å¦‚æœè¿™æ˜¯ç‰©æµå…¬å¸/è´§ä»£å…¬å¸çš„å¹¿å‘Šæˆ–æ¨å¹¿ï¼Œè¿”å› is_lead: false
2. å¦‚æœè¿™æ˜¯çœŸå®çš„å®¢æˆ·åœ¨å¯»æ‰¾ç‰©æµæœåŠ¡ï¼Œè¿”å› is_lead: true
3. åŒ…å«å…·ä½“å‘è´§éœ€æ±‚ï¼ˆç›®çš„åœ°ã€è´§ç‰©ã€æ—¶é—´ï¼‰çš„çº¿ç´¢ä¼˜å…ˆçº§æ›´é«˜

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚"""
        
        try:
            response = await self.think([{"role": "user", "content": prompt}])
            
            # è§£æAIå›å¤
            json_start = response.find("{")
            json_end = response.rfind("}") + 1
            if json_start != -1 and json_end > json_start:
                result = json.loads(response[json_start:json_end])
                
                # å¦‚æœæœ‰é«˜æ„å‘å…³é”®è¯ï¼Œæå‡æ„å‘ç­‰çº§
                if has_high_intent and result.get("is_lead"):
                    if result.get("intent_level") == "low":
                        result["intent_level"] = "medium"
                    elif result.get("intent_level") == "medium":
                        result["intent_level"] = "high"
                
                return result
        except json.JSONDecodeError:
            self.log("AIåˆ†æç»“æœè§£æå¤±è´¥", "warning")
        except Exception as e:
            self.log(f"AIåˆ†æå¼‚å¸¸: {e}", "error")
        
        # å¦‚æœAIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åˆ¤æ–­
        return self._rule_based_analysis(content, has_high_intent)
    
    def _rule_based_analysis(self, content: str, has_high_intent: bool) -> Dict[str, Any]:
        """
        åŸºäºè§„åˆ™çš„ç®€å•åˆ†æï¼ˆAIå¤±è´¥æ—¶çš„å¤‡é€‰ï¼‰
        """
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¹¿å‘Š
        is_ad = any(kw in content for kw in self.AD_FILTER_KEYWORDS)
        if is_ad:
            return {"is_lead": False, "reason": "ç–‘ä¼¼å¹¿å‘Šå†…å®¹"}
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«éœ€æ±‚å…³é”®è¯
        need_keywords = ["æ‰¾", "æ±‚", "æƒ³", "è¦", "éœ€è¦", "æ¨è", "å“ªå®¶", "æ€ä¹ˆé€‰"]
        has_need = any(kw in content for kw in need_keywords)
        
        if has_need:
            return {
                "is_lead": True,
                "confidence": 60 if has_high_intent else 40,
                "intent_level": "high" if has_high_intent else "medium",
                "needs": [],
                "contact_info": {},
                "summary": "è§„åˆ™åŒ¹é…çš„æ½œåœ¨çº¿ç´¢",
                "follow_up_suggestion": "å»ºè®®è¿›ä¸€æ­¥åˆ†æ"
            }
        
        return {"is_lead": False, "reason": "æœªåŒ¹é…åˆ°éœ€æ±‚å…³é”®è¯"}
    
    def _quick_filter(self, content: str) -> bool:
        """
        å¿«é€Ÿè¿‡æ»¤æ˜æ˜¾ä¸æ˜¯çº¿ç´¢çš„å†…å®¹
        """
        # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
        if len(content) < 15:
            return True
        
        # è¿‡æ»¤æ˜æ˜¾çš„å¹¿å‘Š
        ad_strong_keywords = [
            "æ‹›å•†åŠ ç›Ÿ", "ä»£ç†å•†æ‹›å‹Ÿ", "è¯šæ‹›ä»£ç†",
            "æˆ‘å¸ä¸“ä¸š", "æœ¬å…¬å¸ä¸“ä¸š", "æ¬¢è¿æ¥ç”µ",
            "ä¸šåŠ¡åˆä½œ", "æ‹›è˜å¸æœº", "æ‹›è˜ä¸šåŠ¡å‘˜"
        ]
        for kw in ad_strong_keywords:
            if kw in content:
                return True
        
        return False
    
    def _extract_contact_info(self, content: str) -> Dict[str, str]:
        """
        ä»å†…å®¹ä¸­æå–è”ç³»æ–¹å¼
        """
        contact = {
            "phone": "",
            "email": "",
            "wechat": "",
            "qq": "",
            "name": "",
            "company": ""
        }
        
        # æå–æ‰‹æœºå·
        phone_pattern = r'1[3-9]\d{9}'
        phones = re.findall(phone_pattern, content)
        if phones:
            contact["phone"] = phones[0]
        
        # æå–é‚®ç®±
        email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
        emails = re.findall(email_pattern, content)
        if emails:
            contact["email"] = emails[0]
        
        # æå–å¾®ä¿¡å·
        wechat_patterns = [
            r'å¾®ä¿¡[ï¼š:]\s*([a-zA-Z0-9_-]+)',
            r'wx[ï¼š:]\s*([a-zA-Z0-9_-]+)',
            r'V[ï¼š:]\s*([a-zA-Z0-9_-]+)',
            r'WeChat[ï¼š:]\s*([a-zA-Z0-9_-]+)'
        ]
        for pattern in wechat_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                contact["wechat"] = match.group(1)
                break
        
        # æå–QQ
        qq_patterns = [
            r'QQ[ï¼š:]\s*(\d{5,12})',
            r'qq[ï¼š:]\s*(\d{5,12})'
        ]
        for pattern in qq_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                contact["qq"] = match.group(1)
                break
        
        return contact
    
    async def _search_leads(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ¹æ®æŒ‡å®šå‚æ•°æœç´¢çº¿ç´¢
        """
        keywords = input_data.get("keywords", self.FALLBACK_KEYWORDS[:3])
        sources = input_data.get("sources", ["google"])
        
        results = []
        for keyword in keywords:
            for source in sources:
                site_filter = ""
                if source == "weibo":
                    site_filter = "site:weibo.com"
                elif source == "zhihu":
                    site_filter = "site:zhihu.com"
                elif source == "tieba":
                    site_filter = "site:tieba.baidu.com"
                
                query = f"{keyword} {site_filter}".strip()
                search_results = await self._search_with_serper(query)
                results.extend(search_results)
        
        return {"results": results, "count": len(results)}
    
    async def add_keyword(self, keyword: str, keyword_type: str = "general", 
                          platform: str = None, priority: int = 5) -> Dict[str, Any]:
        """
        æ·»åŠ æ–°çš„æœç´¢å…³é”®è¯
        """
        try:
            from app.models.database import async_session_maker
            from sqlalchemy import text
            
            async with async_session_maker() as db:
                await db.execute(
                    text("""
                        INSERT INTO lead_hunt_keywords 
                        (keyword, keyword_type, platform, priority)
                        VALUES (:keyword, :type, :platform, :priority)
                        ON CONFLICT (keyword, platform) DO UPDATE SET
                            priority = :priority,
                            is_active = true,
                            updated_at = NOW()
                    """),
                    {"keyword": keyword, "type": keyword_type, 
                     "platform": platform, "priority": priority}
                )
                await db.commit()
            
            return {"success": True, "keyword": keyword}
        except Exception as e:
            self.log(f"æ·»åŠ å…³é”®è¯å¤±è´¥: {e}", "error")
            return {"success": False, "error": str(e)}


# æ³¨å†ŒAgent
lead_hunter_agent = LeadHunterAgent()
AgentRegistry.register(lead_hunter_agent)
