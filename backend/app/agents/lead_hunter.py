"""
å°çŒ - çº¿ç´¢çŒæ‰‹ & è¯é¢˜å‘ç°è€…
åŒæ¨¡å¼è¿è¡Œï¼š
1. çº¿ç´¢æœç´¢æ¨¡å¼ï¼šæœç´¢äº’è”ç½‘ä¸Šçš„æ½œåœ¨å®¢æˆ·çº¿ç´¢
2. è¯é¢˜å‘ç°æ¨¡å¼ï¼šå‘ç°çƒ­é—¨è¯é¢˜ï¼Œé…åˆå°æ–‡ç”Ÿæˆå›ç­”å†…å®¹å¼•æµ

è¯é¢˜å‘ç°æ¨¡å¼æ”¯æŒï¼š
- æœç´¢çŸ¥ä¹/å°çº¢ä¹¦ç­‰å¹³å°çš„çƒ­é—¨ç‰©æµç›¸å…³è¯é¢˜
- è¯„ä¼°è¯é¢˜ä»·å€¼ï¼ˆæµè§ˆé‡ã€å›ç­”æ•°ã€æ—¶æ•ˆæ€§ï¼‰
- ç”Ÿæˆå›ç­”ç­–ç•¥å»ºè®®
- ä¸å°æ–‡é…åˆä¸€é”®ç”Ÿæˆä¸“ä¸šå›ç­”
"""
from typing import Dict, Any, List, Optional
import json
import re
import asyncio
import httpx
import hashlib
from datetime import datetime, timedelta
from loguru import logger
from app.core.prompts.lead_hunter import SYSTEM_PROMPT as LEAD_HUNTER_SYSTEM_PROMPT

from app.agents.base import BaseAgent, AgentRegistry
from app.models.conversation import AgentType
from app.core.config import settings
from app.core.prompt_utils import sanitize_user_input, wrap_user_content


class LeadHunterAgent(BaseAgent):
    """å°çŒ - çº¿ç´¢çŒæ‰‹ & è¯é¢˜å‘ç°è€…"""
    
    name = "å°çŒ"
    agent_type = AgentType.LEAD_HUNTER
    description = "çº¿ç´¢çŒæ‰‹ & è¯é¢˜å‘ç°è€… - æœç´¢çº¿ç´¢æˆ–å‘ç°çƒ­é—¨è¯é¢˜é…åˆå†…å®¹å¼•æµ"
    
    # å¤‡ç”¨æœç´¢å…³é”®è¯ï¼ˆæ•°æ®åº“å…³é”®è¯ä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰- åªæœç´¢æ¬§æ´²ç›¸å…³
    FALLBACK_KEYWORDS = [
        "æ¬§æ´²è´§ä»£", "æ¬§æ´²ç‰©æµæ¨è", "æ¬§æ´²ç‰©æµæŠ¥ä»·",
        "æ¬§æ´²æ¸…å…³", "æ¬§æ´²æ´¾é€", "æ¬§æ´²åˆ°é—¨",
        "å¾·å›½ç‰©æµ", "æ³•å›½ç‰©æµ", "è‹±å›½ç‰©æµ",
        "å‘è´§åˆ°æ¬§æ´²", "æ¬§æ´²FBA", "æ¬§æ´²åŒæ¸…",
        "å¾·å›½FBA", "è‹±å›½FBA", "æ¬§æ´²å¡æ´¾"
    ]
    
    # æœç´¢å¹³å°é…ç½®
    PLATFORMS = {
        "weibo": {"site": "site:weibo.com", "weight": 3},
        "zhihu": {"site": "site:zhihu.com", "weight": 3},
        "tieba": {"site": "site:tieba.baidu.com", "weight": 2},
        "douyin": {"site": "site:douyin.com", "weight": 2},
        "xiaohongshu": {"site": "site:xiaohongshu.com", "weight": 2},
        "google": {"site": "", "weight": 4}  # å…¨ç½‘æœç´¢
    }
    
    # çº¿ç´¢è´¨é‡åˆ¤æ–­å…³é”®è¯
    HIGH_INTENT_KEYWORDS = [
        "æ€¥", "é©¬ä¸Š", "å°½å¿«", "æŠ¥ä»·", "ä»·æ ¼", "å¤šå°‘é’±",
        "ç«‹å³", "ä»Šå¤©", "æ˜å¤©", "è¿™å‘¨", "æƒ³å‘", "è¦å‘",
        "æ±‚æ¨è", "æœ‰æ²¡æœ‰", "è°çŸ¥é“", "å“ªå®¶å¥½",
        "urgent", "asap", "quote", "price", "how much"
    ]
    
    # å¹¿å‘Šè¿‡æ»¤å…³é”®è¯
    AD_FILTER_KEYWORDS = [
        "åŠ ç›Ÿ", "æ‹›å•†", "ä»£ç†", "å…è´¹è¯•ç”¨", "é™æ—¶ä¼˜æƒ ",
        "æ¬¢è¿å’¨è¯¢", "ä¸“ä¸šç‰©æµ", "æˆ‘å¸", "æˆ‘ä»¬å…¬å¸",
        "è”ç³»ç”µè¯", "ç‚¹å‡»å’¨è¯¢", "åœ¨çº¿å®¢æœ", "æ‹›ä»£ç†",
        "è¯šæ‹›", "ç«çƒ­æ‹›å•†"
    ]
    
    def _build_system_prompt(self) -> str:
        return LEAD_HUNTER_SYSTEM_PROMPT
    
    async def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†çº¿ç´¢æœç´¢æˆ–è¯é¢˜å‘ç°ä»»åŠ¡
        
        Args:
            input_data: {
                "action": "search" | "analyze" | "hunt" | "smart_hunt" | 
                          "discover_topics" | "analyze_topic" | "get_topic_stats" |
                          "discover_products" | "get_product_stats",
                "source": "æœç´¢æ¥æº",
                "content": "è¦åˆ†æçš„å†…å®¹",
                "keywords": ["è‡ªå®šä¹‰å…³é”®è¯"],
                "max_keywords": æœ€å¤§å…³é”®è¯æ•°é‡,
                "max_results": æœ€å¤§ç»“æœæ•°é‡
            }
        """
        action = input_data.get("action", "smart_hunt")
        
        # å¼€å§‹ä»»åŠ¡ä¼šè¯ï¼ˆå®æ—¶ç›´æ’­ï¼‰
        await self.start_task_session(action, f"çº¿ç´¢æœç´¢ä»»åŠ¡: {action}")
        
        try:
            # çº¿ç´¢æœç´¢æ¨¡å¼
            if action == "search":
                result = await self._search_leads(input_data)
            elif action == "analyze":
                result = await self._analyze_content(input_data)
            elif action == "hunt":
                result = await self._full_hunt(input_data)
            elif action == "smart_hunt":
                result = await self._smart_hunt(input_data)
            elif action == "get_stats":
                result = await self._get_hunt_stats()
            # è¯é¢˜å‘ç°æ¨¡å¼
            elif action == "discover_topics":
                result = await self._discover_topics(input_data)
            elif action == "analyze_topic":
                result = await self._analyze_topic_value(input_data)
            elif action == "get_topic_stats":
                result = await self._get_topic_stats()
            elif action == "generate_answer":
                result = await self._generate_answer(input_data)
            # äº§å“è¶‹åŠ¿å‘ç°æ¨¡å¼ï¼ˆå†…å®¹å¼•æµ + å¸‚åœºæ´å¯Ÿï¼‰
            elif action == "discover_products":
                result = await self._discover_product_trends(input_data)
            elif action == "get_product_stats":
                result = await self._get_product_stats()
            else:
                result = {"error": f"æœªçŸ¥æ“ä½œ: {action}"}
            
            await self.end_task_session(f"å®Œæˆ{action}ä»»åŠ¡")
            return result
        except Exception as e:
            await self.end_task_session(error_message=str(e))
            raise
    
    async def _smart_hunt(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ™ºèƒ½çº¿ç´¢ç‹©çŒ - 24å°æ—¶è‡ªåŠ¨è¿è¡Œç‰ˆæœ¬
        - è‡ªåŠ¨ä»æ•°æ®åº“è·å–å¾…æœç´¢å…³é”®è¯
        - æ™ºèƒ½é€‰æ‹©æœç´¢å¹³å°
        - è‡ªåŠ¨å»é‡å’Œè®°å½•
        - è¿½è¸ªæœç´¢æ•ˆæœ
        - åªæœç´¢æœ€è¿‘1ä¸ªæœˆå†…çš„å†…å®¹ï¼ˆç¡®ä¿çº¿ç´¢æ—¶æ•ˆæ€§ï¼‰
        """
        # å¼€å§‹ä»»åŠ¡ä¼šè¯ï¼ˆå®æ—¶ç›´æ’­ï¼‰
        await self.start_task_session("smart_hunt", "æ™ºèƒ½çº¿ç´¢ç‹©çŒ - æœç´¢äº’è”ç½‘æ½œåœ¨å®¢æˆ·")
        
        self.log("ğŸ¯ å¼€å§‹æ™ºèƒ½çº¿ç´¢ç‹©çŒä»»åŠ¡ï¼ˆä»…æœç´¢æœ€è¿‘1ä¸ªæœˆå†…çš„çº¿ç´¢ï¼‰...")
        start_time = datetime.now()
        
        # æ£€æŸ¥APIé…ç½®
        api_key = getattr(settings, 'SERPER_API_KEY', None)
        if not api_key:
            self.log("Serper APIæœªé…ç½®ï¼Œæ— æ³•è¿›è¡Œæœç´¢", "error")
            await self.log_error("Serper APIæœªé…ç½®", "è¯·åœ¨ç³»ç»Ÿè®¾ç½®ä¸­é…ç½®APIå¯†é’¥")
            await self.end_task_session(error_message="APIæœªé…ç½®")
            return {
                "error": "æœç´¢APIæœªé…ç½®",
                "message": "è¯·åœ¨ç³»ç»Ÿè®¾ç½®ä¸­é…ç½® SERPER_API_KEY ä»¥å¯ç”¨çº¿ç´¢æœç´¢åŠŸèƒ½",
                "hunt_time": datetime.now().isoformat(),
                "sources_searched": [],
                "leads_found": [],
                "total_leads": 0
            }
        
        results = {
            "hunt_time": datetime.now().isoformat(),
            "hunt_mode": "smart_24h",
            "sources_searched": [],
            "leads_found": [],
            "total_leads": 0,
            "high_intent_leads": 0,
            "keywords_used": [],
            "search_queries": [],
            "new_urls": 0,
            "duplicate_urls": 0,
            "stats": {}
        }
        
        try:
            from app.models.database import async_session_maker
            from sqlalchemy import text
            
            async with async_session_maker() as db:
                # 1. è·å–å¾…æœç´¢çš„å…³é”®è¯ï¼ˆä¼˜å…ˆçº§é«˜ã€æ•ˆæœå¥½ã€å†·å´æ—¶é—´å·²è¿‡ï¼‰
                max_keywords = input_data.get("max_keywords", 5)
                
                keyword_result = await db.execute(
                    text("""
                        SELECT id, keyword, keyword_type, platform, priority, success_rate
                        FROM lead_hunt_keywords
                        WHERE is_active = true
                        AND (next_search_after IS NULL OR next_search_after <= NOW())
                        ORDER BY 
                            priority DESC,
                            success_rate DESC,
                            last_searched_at ASC NULLS FIRST
                        LIMIT :limit
                    """),
                    {"limit": max_keywords}
                )
                keywords_data = keyword_result.fetchall()
                
                # å¦‚æœæ•°æ®åº“æ²¡æœ‰å…³é”®è¯ï¼Œä½¿ç”¨å¤‡ç”¨å…³é”®è¯
                if not keywords_data:
                    self.log("æ•°æ®åº“æ— å¯ç”¨å…³é”®è¯ï¼Œä½¿ç”¨å¤‡ç”¨å…³é”®è¯")
                    keywords_to_use = self.FALLBACK_KEYWORDS[:max_keywords]
                    keywords_data = [(None, kw, 'fallback', None, 5, 0) for kw in keywords_to_use]
                
                self.log(f"æœ¬æ¬¡å°†ä½¿ç”¨ {len(keywords_data)} ä¸ªå…³é”®è¯æœç´¢")
                await self.log_live_step("info", f"å‡†å¤‡æœç´¢ {len(keywords_data)} ä¸ªå…³é”®è¯", 
                    f"å…³é”®è¯: {', '.join([k[1] for k in keywords_data[:5]])}")
                
                all_raw_results = []
                
                # 2. å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œæœç´¢
                for kw_data in keywords_data:
                    kw_id, keyword, kw_type, kw_platform, priority, success_rate = kw_data
                    results["keywords_used"].append(keyword)
                    
                    # ç¡®å®šæœç´¢å¹³å°
                    if kw_platform:
                        platforms_to_search = [(kw_platform, self.PLATFORMS.get(kw_platform, {}).get("site", ""))]
                    else:
                        # æ ¹æ®å½“å‰æ—¶é—´æ™ºèƒ½é€‰æ‹©å¹³å°
                        platforms_to_search = self._select_platforms_by_time()
                    
                    keyword_leads = 0
                    keyword_high_intent = 0
                    
                    for platform_name, site_filter in platforms_to_search:
                        try:
                            query = f"{keyword} {site_filter}".strip()
                            self.log(f"ğŸ” æœç´¢: {query}")
                            results["search_queries"].append(query)
                            
                            # è®°å½•æœç´¢æ­¥éª¤ï¼ˆå®æ—¶ç›´æ’­ï¼‰
                            await self.log_search(keyword, platform_name, {"query": query})
                            
                            search_results = await self._search_with_serper(query)
                            
                            if search_results:
                                results["sources_searched"].append(platform_name)
                                
                                for item in search_results:
                                    url = item.get("url", "")
                                    if not url:
                                        continue
                                    
                                    # æ£€æŸ¥URLæ˜¯å¦å·²æœç´¢è¿‡
                                    url_hash = hashlib.md5(url.encode()).hexdigest()
                                    
                                    existing_url = await db.execute(
                                        text("""
                                            SELECT id, is_lead FROM lead_hunt_searched_urls
                                            WHERE url_hash = :hash
                                        """),
                                        {"hash": url_hash}
                                    )
                                    existing = existing_url.fetchone()
                                    
                                    if existing:
                                        results["duplicate_urls"] += 1
                                        continue
                                    
                                    results["new_urls"] += 1
                                    
                                    item["platform"] = platform_name
                                    item["keyword"] = keyword
                                    item["keyword_id"] = kw_id
                                    item["url_hash"] = url_hash
                                    all_raw_results.append(item)
                            
                            # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                            await asyncio.sleep(0.5)
                            
                        except Exception as e:
                            self.log(f"æœç´¢å¤±è´¥ ({platform_name}, {keyword}): {e}", "error")
                    
                    # æ›´æ–°å…³é”®è¯ç»Ÿè®¡ï¼ˆåœ¨åˆ†æå®Œæ‰€æœ‰ç»“æœåæ›´æ–°ï¼‰
                
                self.log(f"ğŸ“Š è·å– {len(all_raw_results)} æ¡æ–°URLå¾…åˆ†æ")
                await self.log_live_step("info", f"è·å– {len(all_raw_results)} æ¡æ–°URL", "å¼€å§‹AIåˆ†æç­›é€‰")
                
                # 3. åˆ†ææ¯ä¸ªæœç´¢ç»“æœ
                max_results = input_data.get("max_results", 30)
                keyword_stats = {}  # è®°å½•æ¯ä¸ªå…³é”®è¯çš„æ•ˆæœ
                analyzed_count = 0
                
                for item in all_raw_results[:max_results]:
                    try:
                        content = f"{item.get('title', '')} {item.get('content', '')}"
                        url = item.get("url", "")
                        url_hash = item.get("url_hash", "")
                        keyword = item.get("keyword", "")
                        keyword_id = item.get("keyword_id")
                        platform = item.get("platform", "google")
                        
                        # å¿«é€Ÿè¿‡æ»¤
                        if self._quick_filter(content):
                            # è®°å½•ä¸ºéçº¿ç´¢URL
                            await db.execute(
                                text("""
                                    INSERT INTO lead_hunt_searched_urls 
                                    (url_hash, url, source_keyword, platform, is_lead)
                                    VALUES (:hash, :url, :keyword, :platform, false)
                                    ON CONFLICT (url_hash) DO NOTHING
                                """),
                                {"hash": url_hash, "url": url, "keyword": keyword, "platform": platform}
                            )
                            continue
                        
                        # è®°å½•æ­£åœ¨åˆ†æçš„URLï¼ˆå®æ—¶ç›´æ’­ï¼‰
                        analyzed_count += 1
                        await self.log_fetch(url, item.get("title", ""), {"platform": platform})
                        
                        # AIæ·±åº¦åˆ†æ
                        await self.log_think("åˆ¤æ–­æ˜¯å¦ä¸ºæ½œåœ¨å®¢æˆ·çº¿ç´¢", content[:100])
                        analysis = await self._analyze_content({
                            "content": content,
                            "source": platform,
                            "url": url
                        })
                        
                        is_lead = analysis.get("is_lead", False)
                        intent_level = analysis.get("intent_level", "low")
                        is_high_intent = intent_level == "high"
                        
                        # åˆå§‹åŒ–å…³é”®è¯ç»Ÿè®¡
                        if keyword not in keyword_stats:
                            keyword_stats[keyword] = {"id": keyword_id, "leads": 0, "high_intent": 0}
                        
                        if is_lead:
                            # è®°å½•å‘ç°çº¿ç´¢ï¼ˆå®æ—¶ç›´æ’­ï¼‰
                            await self.log_result(
                                f"ğŸ¯ å‘ç°æ½œåœ¨çº¿ç´¢!", 
                                f"æ„å‘ç­‰çº§: {intent_level}, æ¥æº: {platform}",
                                {"url": url, "intent_level": intent_level}
                            )
                            
                            # æå–è”ç³»æ–¹å¼
                            contact_info = analysis.get("contact_info", {})
                            extracted_contact = self._extract_contact_info(content)
                            for key, value in extracted_contact.items():
                                if value and not contact_info.get(key):
                                    contact_info[key] = value
                            
                            lead_data = {
                                "title": item.get("title", ""),
                                "content": content,
                                "url": url,
                                "source": platform,
                                "keyword": keyword,
                                "found_at": datetime.now().isoformat(),
                                "is_lead": True,
                                "confidence": analysis.get("confidence", 50),
                                "intent_level": intent_level,
                                "lead_type": analysis.get("lead_type", ""),
                                "needs": analysis.get("needs", []),
                                "contact_info": contact_info,
                                "summary": analysis.get("summary", ""),
                                "follow_up_suggestion": analysis.get("follow_up_suggestion", "")
                            }
                            
                            results["leads_found"].append(lead_data)
                            results["total_leads"] += 1
                            keyword_stats[keyword]["leads"] += 1
                            
                            if is_high_intent:
                                results["high_intent_leads"] += 1
                                keyword_stats[keyword]["high_intent"] += 1
                            
                            # æ£€æµ‹çº¿ç´¢è¯­è¨€
                            from app.services.language_detector import language_detector
                            lead_language = language_detector.detect_customer_language(
                                name=lead_data.get("contact_name"),
                                email=lead_data.get("email"),
                                company=lead_data.get("company"),
                                message=content  # ç”¨åŸå§‹å†…å®¹æ£€æµ‹
                            )
                            
                            # ä¿å­˜çº¿ç´¢åˆ°æ•°æ®åº“
                            lead_insert = await db.execute(
                                text("""
                                    INSERT INTO leads 
                                    (source, source_url, source_content, content, 
                                     ai_confidence, intent_level, ai_summary, ai_suggestion,
                                     needs, status, language, created_at)
                                    VALUES (:source, :url, :raw_content, :content, 
                                            :confidence, :level, :summary, :suggestion,
                                            :needs, 'new', :language, NOW())
                                    ON CONFLICT (source_url) DO NOTHING
                                    RETURNING id
                                """),
                                {
                                    "source": platform,
                                    "url": url,
                                    "raw_content": content,
                                    "content": json.dumps(lead_data, ensure_ascii=False),
                                    "confidence": analysis.get("confidence", 50) / 100.0,
                                    "level": {"high": "high", "medium": "medium", "low": "low"}.get(intent_level, "unknown"),
                                    "summary": analysis.get("summary", ""),
                                    "suggestion": analysis.get("follow_up_suggestion", ""),
                                    "needs": analysis.get("needs", []),
                                    "language": lead_language
                                }
                            )
                            lead_row = lead_insert.fetchone()
                            lead_id = lead_row[0] if lead_row else None
                            
                            # è®°å½•å·²æœç´¢URLï¼ˆæ ‡è®°ä¸ºçº¿ç´¢ï¼‰
                            await db.execute(
                                text("""
                                    INSERT INTO lead_hunt_searched_urls 
                                    (url_hash, url, source_keyword, platform, is_lead, lead_id)
                                    VALUES (:hash, :url, :keyword, :platform, true, :lead_id)
                                    ON CONFLICT (url_hash) DO NOTHING
                                """),
                                {"hash": url_hash, "url": url, "keyword": keyword, 
                                 "platform": platform, "lead_id": lead_id}
                            )
                        else:
                            # è®°å½•ä¸ºéçº¿ç´¢URL
                            await db.execute(
                                text("""
                                    INSERT INTO lead_hunt_searched_urls 
                                    (url_hash, url, source_keyword, platform, is_lead)
                                    VALUES (:hash, :url, :keyword, :platform, false)
                                    ON CONFLICT (url_hash) DO NOTHING
                                """),
                                {"hash": url_hash, "url": url, "keyword": keyword, "platform": platform}
                            )
                            
                    except Exception as e:
                        self.log(f"åˆ†æå†…å®¹å¤±è´¥: {e}", "error")
                
                # 4. æ›´æ–°å…³é”®è¯æ•ˆæœç»Ÿè®¡
                for keyword, stats in keyword_stats.items():
                    if stats["id"]:
                        await db.execute(
                            text("""
                                SELECT update_keyword_stats(:kw_id, :leads, :high_intent)
                            """),
                            {"kw_id": stats["id"], "leads": stats["leads"], "high_intent": stats["high_intent"]}
                        )
                
                # 5. è®°å½•æœç´¢å†å²
                duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
                
                await db.execute(
                    text("""
                        INSERT INTO lead_hunt_history 
                        (keyword, search_query, results_count, leads_found, high_intent_leads, duration_ms)
                        VALUES (:keyword, :query, :results, :leads, :high_intent, :duration)
                    """),
                    {
                        "keyword": ", ".join(results["keywords_used"]),
                        "query": ", ".join(results["search_queries"][:10]),  # åªè®°å½•å‰10ä¸ª
                        "results": len(all_raw_results),
                        "leads": results["total_leads"],
                        "high_intent": results["high_intent_leads"],
                        "duration": duration_ms
                    }
                )
                
                # 6. æ›´æ–°æ¯æ—¥ç»Ÿè®¡
                today = datetime.now().date()
                await db.execute(
                    text("""
                        INSERT INTO lead_hunt_stats 
                        (stat_date, total_searches, total_results, total_leads, high_intent_leads, unique_urls)
                        VALUES (:date, 1, :results, :leads, :high_intent, :urls)
                        ON CONFLICT (stat_date) DO UPDATE SET
                            total_searches = lead_hunt_stats.total_searches + 1,
                            total_results = lead_hunt_stats.total_results + :results,
                            total_leads = lead_hunt_stats.total_leads + :leads,
                            high_intent_leads = lead_hunt_stats.high_intent_leads + :high_intent,
                            unique_urls = lead_hunt_stats.unique_urls + :urls,
                            updated_at = NOW()
                    """),
                    {
                        "date": today,
                        "results": len(all_raw_results),
                        "leads": results["total_leads"],
                        "high_intent": results["high_intent_leads"],
                        "urls": results["new_urls"]
                    }
                )
                
                # 7. æ›´æ–°å°çŒçš„ä»»åŠ¡ç»Ÿè®¡
                await db.execute(
                    text("""
                        UPDATE ai_agents
                        SET tasks_completed_today = tasks_completed_today + 1,
                            tasks_completed_total = tasks_completed_total + 1,
                            last_active_at = NOW(),
                            updated_at = NOW()
                        WHERE agent_type = 'lead_hunter'
                    """)
                )
                
                await db.commit()
        
        except Exception as e:
            self.log(f"æ™ºèƒ½ç‹©çŒå‡ºé”™: {e}", "error")
            results["error"] = str(e)
            await self.log_error(str(e), "æ™ºèƒ½ç‹©çŒä»»åŠ¡å‡ºé”™")
            await self.end_task_session(error_message=str(e))
            return results
        
        # å»é‡sources_searched
        results["sources_searched"] = list(set(results["sources_searched"]))
        
        duration = (datetime.now() - start_time).total_seconds()
        results["stats"] = {
            "duration_seconds": round(duration, 2),
            "keywords_count": len(results["keywords_used"]),
            "queries_count": len(results["search_queries"]),
            "new_urls_analyzed": results["new_urls"],
            "duplicate_urls_skipped": results["duplicate_urls"]
        }
        
        self.log(f"âœ… æ™ºèƒ½ç‹©çŒå®Œæˆï¼è€—æ—¶{duration:.1f}ç§’ï¼Œæ–°URL {results['new_urls']} æ¡ï¼Œ"
                 f"å‘ç°çº¿ç´¢ {results['total_leads']} æ¡ï¼Œé«˜æ„å‘ {results['high_intent_leads']} æ¡")
        
        # ç»“æŸä»»åŠ¡ä¼šè¯ï¼ˆå®æ—¶ç›´æ’­ï¼‰
        await self.end_task_session(
            f"å‘ç° {results['total_leads']} æ¡çº¿ç´¢ï¼Œå…¶ä¸­é«˜æ„å‘ {results['high_intent_leads']} æ¡"
        )
        
        return results
    
    def _select_platforms_by_time(self) -> List[tuple]:
        """
        æ ¹æ®å½“å‰æ—¶é—´æ™ºèƒ½é€‰æ‹©æœç´¢å¹³å°
        ä¸åŒæ—¶é—´æ®µç”¨æˆ·æ´»è·ƒçš„å¹³å°ä¸åŒ
        """
        current_hour = datetime.now().hour
        
        # æ·±å¤œ/å‡Œæ™¨ (0-6ç‚¹) - æœç´¢é‡è¾ƒå°‘ï¼Œä¸»è¦æœGoogle
        if 0 <= current_hour < 6:
            return [
                ("google", ""),
                ("zhihu", self.PLATFORMS["zhihu"]["site"])
            ]
        # æ—©ä¸Š (6-9ç‚¹) - å¾®åšæ´»è·ƒ
        elif 6 <= current_hour < 9:
            return [
                ("weibo", self.PLATFORMS["weibo"]["site"]),
                ("google", "")
            ]
        # ä¸Šåˆå·¥ä½œæ—¶é—´ (9-12ç‚¹) - å…¨é¢æœç´¢
        elif 9 <= current_hour < 12:
            return [
                ("google", ""),
                ("zhihu", self.PLATFORMS["zhihu"]["site"]),
                ("weibo", self.PLATFORMS["weibo"]["site"])
            ]
        # åˆä¼‘æ—¶é—´ (12-14ç‚¹) - ç¤¾äº¤å¹³å°æ´»è·ƒ
        elif 12 <= current_hour < 14:
            return [
                ("weibo", self.PLATFORMS["weibo"]["site"]),
                ("xiaohongshu", self.PLATFORMS["xiaohongshu"]["site"]),
                ("google", "")
            ]
        # ä¸‹åˆå·¥ä½œæ—¶é—´ (14-18ç‚¹) - å…¨é¢æœç´¢
        elif 14 <= current_hour < 18:
            return [
                ("google", ""),
                ("zhihu", self.PLATFORMS["zhihu"]["site"]),
                ("tieba", self.PLATFORMS["tieba"]["site"])
            ]
        # æ™šé—´ (18-22ç‚¹) - ç¤¾äº¤å¹³å°æœ€æ´»è·ƒ
        elif 18 <= current_hour < 22:
            return [
                ("weibo", self.PLATFORMS["weibo"]["site"]),
                ("zhihu", self.PLATFORMS["zhihu"]["site"]),
                ("douyin", self.PLATFORMS["douyin"]["site"]),
                ("google", "")
            ]
        # æ·±å¤œ (22-24ç‚¹) - çŸ¥ä¹å¤œçŒ«å­æ´»è·ƒ
        else:
            return [
                ("zhihu", self.PLATFORMS["zhihu"]["site"]),
                ("google", "")
            ]
    
    async def _get_hunt_stats(self) -> Dict[str, Any]:
        """
        è·å–æœç´¢ç»Ÿè®¡æ•°æ®
        """
        try:
            from app.models.database import async_session_maker
            from sqlalchemy import text
            
            async with async_session_maker() as db:
                # ä»Šæ—¥ç»Ÿè®¡
                today_result = await db.execute(
                    text("""
                        SELECT total_searches, total_results, total_leads, 
                               high_intent_leads, unique_urls
                        FROM lead_hunt_stats
                        WHERE stat_date = CURRENT_DATE
                    """)
                )
                today = today_result.fetchone()
                
                # æœ¬å‘¨ç»Ÿè®¡
                week_result = await db.execute(
                    text("""
                        SELECT SUM(total_searches), SUM(total_results), 
                               SUM(total_leads), SUM(high_intent_leads)
                        FROM lead_hunt_stats
                        WHERE stat_date >= CURRENT_DATE - INTERVAL '7 days'
                    """)
                )
                week = week_result.fetchone()
                
                # æœ€ä½³å…³é”®è¯
                best_kw_result = await db.execute(
                    text("""
                        SELECT keyword, leads_found, success_rate
                        FROM lead_hunt_keywords
                        WHERE is_active = true AND search_count > 0
                        ORDER BY success_rate DESC
                        LIMIT 5
                    """)
                )
                best_keywords = best_kw_result.fetchall()
                
                return {
                    "today": {
                        "searches": today[0] if today else 0,
                        "results": today[1] if today else 0,
                        "leads": today[2] if today else 0,
                        "high_intent": today[3] if today else 0,
                        "unique_urls": today[4] if today else 0
                    },
                    "this_week": {
                        "searches": week[0] if week else 0,
                        "results": week[1] if week else 0,
                        "leads": week[2] if week else 0,
                        "high_intent": week[3] if week else 0
                    },
                    "best_keywords": [
                        {"keyword": kw[0], "leads": kw[1], "success_rate": round(kw[2] * 100, 1)}
                        for kw in best_keywords
                    ]
                }
        except Exception as e:
            self.log(f"è·å–ç»Ÿè®¡å¤±è´¥: {e}", "error")
            return {"error": str(e)}
    
    async def _full_hunt(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å®Œæ•´çš„çº¿ç´¢ç‹©çŒæµç¨‹ - ä½¿ç”¨Serper APIæœç´¢
        (ä¿ç•™åŸæœ‰æ–¹æ³•ä»¥ä¿æŒå…¼å®¹æ€§)
        - åªæœç´¢æœ€è¿‘1ä¸ªæœˆå†…çš„å†…å®¹
        """
        self.log("å¼€å§‹çº¿ç´¢ç‹©çŒä»»åŠ¡ï¼ˆä»…æœç´¢æœ€è¿‘1ä¸ªæœˆå†…çš„çº¿ç´¢ï¼‰...")
        
        # æ£€æŸ¥APIé…ç½®
        api_key = getattr(settings, 'SERPER_API_KEY', None)
        if not api_key:
            self.log("Serper APIæœªé…ç½®ï¼Œæ— æ³•è¿›è¡Œæœç´¢", "error")
            return {
                "error": "æœç´¢APIæœªé…ç½®",
                "message": "è¯·åœ¨ç³»ç»Ÿè®¾ç½®ä¸­é…ç½® SERPER_API_KEY ä»¥å¯ç”¨çº¿ç´¢æœç´¢åŠŸèƒ½",
                "hunt_time": datetime.now().isoformat(),
                "sources_searched": [],
                "leads_found": [],
                "total_leads": 0
            }
        
        results = {
            "hunt_time": datetime.now().isoformat(),
            "sources_searched": [],
            "leads_found": [],
            "total_leads": 0,
            "high_intent_leads": 0,
            "search_queries": []
        }
        
        # è·å–è‡ªå®šä¹‰å…³é”®è¯æˆ–ä½¿ç”¨é»˜è®¤å…³é”®è¯
        keywords = input_data.get("keywords", self.FALLBACK_KEYWORDS[:6])
        
        # å®šä¹‰æœç´¢å¹³å°å’Œå¯¹åº”çš„siteé™å®š
        platforms = [
            ("weibo", "site:weibo.com"),
            ("zhihu", "site:zhihu.com"),
            ("tieba", "site:tieba.baidu.com"),
            ("google", "")  # å…¨ç½‘æœç´¢
        ]
        
        all_raw_results = []
        
        # å¯¹æ¯ä¸ªå…³é”®è¯å’Œå¹³å°ç»„åˆè¿›è¡Œæœç´¢
        for keyword in keywords[:3]:  # é™åˆ¶æœç´¢æ¬¡æ•°ï¼Œæ§åˆ¶APIè°ƒç”¨
            for platform_name, site_filter in platforms:
                try:
                    query = f"{keyword} {site_filter}".strip()
                    self.log(f"æœç´¢: {query}")
                    results["search_queries"].append(query)
                    
                    search_results = await self._search_with_serper(query)
                    
                    if search_results:
                        results["sources_searched"].append(platform_name)
                        for item in search_results:
                            item["platform"] = platform_name
                            item["keyword"] = keyword
                            all_raw_results.append(item)
                    
                    # é¿å…è¯·æ±‚è¿‡å¿«
                    await asyncio.sleep(0.5)
                    
                except Exception as e:
                    self.log(f"æœç´¢å¤±è´¥ ({platform_name}, {keyword}): {e}", "error")
        
        # å»é‡ï¼ˆæ ¹æ®URLï¼‰
        seen_urls = set()
        unique_results = []
        for item in all_raw_results:
            url = item.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(item)
        
        self.log(f"å…±è·å– {len(unique_results)} æ¡å»é‡åçš„æœç´¢ç»“æœ")
        
        # åˆ†ææ¯ä¸ªæœç´¢ç»“æœ
        for item in unique_results[:20]:  # é™åˆ¶åˆ†ææ•°é‡
            try:
                content = f"{item.get('title', '')} {item.get('content', '')}"
                
                # å¿«é€Ÿè¿‡æ»¤
                if self._quick_filter(content):
                    continue
                
                # AIæ·±åº¦åˆ†æ
                analysis = await self._analyze_content({
                    "content": content,
                    "source": item.get("platform", "google"),
                    "url": item.get("url", "")
                })
                
                if analysis.get("is_lead"):
                    # æå–è”ç³»æ–¹å¼
                    contact_info = analysis.get("contact_info", {})
                    extracted_contact = self._extract_contact_info(content)
                    # åˆå¹¶è”ç³»ä¿¡æ¯
                    for key, value in extracted_contact.items():
                        if value and not contact_info.get(key):
                            contact_info[key] = value
                    
                    lead_data = {
                        "title": item.get("title", ""),
                        "content": content,
                        "url": item.get("url", ""),
                        "source": item.get("platform", "google"),
                        "keyword": item.get("keyword", ""),
                        "found_at": datetime.now().isoformat(),
                        "is_lead": True,
                        "confidence": analysis.get("confidence", 50),
                        "intent_level": analysis.get("intent_level", "medium"),
                        "lead_type": analysis.get("lead_type", ""),
                        "needs": analysis.get("needs", []),
                        "contact_info": contact_info,
                        "summary": analysis.get("summary", ""),
                        "follow_up_suggestion": analysis.get("follow_up_suggestion", "")
                    }
                    
                    results["leads_found"].append(lead_data)
                    results["total_leads"] += 1
                    
                    if analysis.get("intent_level") == "high":
                        results["high_intent_leads"] += 1
                        
            except Exception as e:
                self.log(f"åˆ†æå†…å®¹å¤±è´¥: {e}", "error")
        
        # å»é‡sources_searched
        results["sources_searched"] = list(set(results["sources_searched"]))
        
        self.log(f"çº¿ç´¢ç‹©çŒå®Œæˆï¼æ‰¾åˆ° {results['total_leads']} æ¡çº¿ç´¢ï¼Œå…¶ä¸­é«˜æ„å‘ {results['high_intent_leads']} æ¡")
        
        return results
    
    async def _search_with_serper(self, query: str, time_range: str = "m") -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨Serper APIæœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            time_range: æ—¶é—´èŒƒå›´é™åˆ¶
                - "d": è¿‡å»ä¸€å¤©
                - "w": è¿‡å»ä¸€å‘¨  
                - "m": è¿‡å»ä¸€ä¸ªæœˆï¼ˆé»˜è®¤ï¼‰
                - "y": è¿‡å»ä¸€å¹´
                - None: ä¸é™åˆ¶æ—¶é—´
        """
        api_key = getattr(settings, 'SERPER_API_KEY', None)
        if not api_key:
            return []
        
        try:
            # æ„å»ºæœç´¢å‚æ•°
            search_params = {
                "q": query,
                "gl": "cn",
                "hl": "zh-cn",
                "num": 10
            }
            
            # æ·»åŠ æ—¶é—´é™åˆ¶ï¼štbså‚æ•°æ§åˆ¶æœç´¢ç»“æœæ—¶é—´èŒƒå›´
            # qdr:d = è¿‡å»ä¸€å¤©, qdr:w = è¿‡å»ä¸€å‘¨, qdr:m = è¿‡å»ä¸€ä¸ªæœˆ, qdr:y = è¿‡å»ä¸€å¹´
            if time_range:
                search_params["tbs"] = f"qdr:{time_range}"
                self.log(f"ğŸ• æœç´¢æ—¶é—´é™åˆ¶: è¿‡å»{'ä¸€å¤©' if time_range == 'd' else 'ä¸€å‘¨' if time_range == 'w' else 'ä¸€ä¸ªæœˆ' if time_range == 'm' else 'ä¸€å¹´'}")
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    "https://google.serper.dev/search",
                    headers={
                        "X-API-KEY": api_key,
                        "Content-Type": "application/json"
                    },
                    json=search_params
                )
                
                if response.status_code == 200:
                    data = response.json()
                    results = []
                    
                    for item in data.get("organic", []):
                        results.append({
                            "title": item.get("title", ""),
                            "content": item.get("snippet", ""),
                            "url": item.get("link", ""),
                            "position": item.get("position", 0)
                        })
                    
                    return results
                else:
                    self.log(f"Serper APIè¿”å›é”™è¯¯: {response.status_code}", "error")
                    
        except Exception as e:
            self.log(f"Serperæœç´¢å¼‚å¸¸: {e}", "error")
        
        return []
    
    # å…¬å¼€æ–¹æ³•ä¾›å¤–éƒ¨è°ƒç”¨
    async def search_with_serper(self, query: str, time_range: str = "m") -> List[Dict[str, Any]]:
        """
        å…¬å¼€çš„Serperæœç´¢æ–¹æ³•
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            time_range: æ—¶é—´èŒƒå›´é™åˆ¶ï¼ˆé»˜è®¤1ä¸ªæœˆï¼‰
                - "d": è¿‡å»ä¸€å¤©
                - "w": è¿‡å»ä¸€å‘¨
                - "m": è¿‡å»ä¸€ä¸ªæœˆï¼ˆé»˜è®¤ï¼‰
                - "y": è¿‡å»ä¸€å¹´
                - None: ä¸é™åˆ¶æ—¶é—´
        """
        return await self._search_with_serper(query, time_range)
    
    async def _analyze_content(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä½¿ç”¨AIåˆ†æå†…å®¹æ˜¯å¦æ˜¯æ½œåœ¨å®¢æˆ·
        """
        content = input_data.get("content", "")
        source = input_data.get("source", "unknown")
        url = input_data.get("url", "")
        
        if not content:
            return {"is_lead": False, "reason": "å†…å®¹ä¸ºç©º"}
        
        # å¿«é€Ÿè§„åˆ™åˆ¤æ–­
        # æ£€æŸ¥æ˜¯å¦åŒ…å«é«˜æ„å‘å…³é”®è¯
        has_high_intent = any(kw in content for kw in self.HIGH_INTENT_KEYWORDS)
        
        # ç”¨AIæ·±åº¦åˆ†æï¼ˆæ¸…ç†ç”¨æˆ·è¾“å…¥é˜²æ­¢æ³¨å…¥ï¼‰
        safe_content = sanitize_user_input(content, max_length=5000)
        safe_url = sanitize_user_input(url, max_length=500)
        safe_source = sanitize_user_input(source, max_length=100)
        
        prompt = f"""è¯·åˆ†æä»¥ä¸‹å†…å®¹æ˜¯å¦æ˜¯æ½œåœ¨çš„ç‰©æµå®¢æˆ·çº¿ç´¢ï¼š

æ¥æºå¹³å°ï¼š{safe_source}
URLï¼š{safe_url}
å†…å®¹ï¼š{safe_content}

æ³¨æ„ï¼š
1. å¦‚æœè¿™æ˜¯ç‰©æµå…¬å¸/è´§ä»£å…¬å¸çš„å¹¿å‘Šæˆ–æ¨å¹¿ï¼Œè¿”å› is_lead: false
2. å¦‚æœè¿™æ˜¯çœŸå®çš„å®¢æˆ·åœ¨å¯»æ‰¾ç‰©æµæœåŠ¡ï¼Œè¿”å› is_lead: true
3. åŒ…å«å…·ä½“å‘è´§éœ€æ±‚ï¼ˆç›®çš„åœ°ã€è´§ç‰©ã€æ—¶é—´ï¼‰çš„çº¿ç´¢ä¼˜å…ˆçº§æ›´é«˜

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚"""
        
        try:
            response = await self.think([{"role": "user", "content": prompt}])
            
            # è§£æAIå›å¤
            json_start = response.find("{")
            json_end = response.rfind("}") + 1
            if json_start != -1 and json_end > json_start:
                result = json.loads(response[json_start:json_end])
                
                # å¦‚æœæœ‰é«˜æ„å‘å…³é”®è¯ï¼Œæå‡æ„å‘ç­‰çº§
                if has_high_intent and result.get("is_lead"):
                    if result.get("intent_level") == "low":
                        result["intent_level"] = "medium"
                    elif result.get("intent_level") == "medium":
                        result["intent_level"] = "high"
                
                return result
        except json.JSONDecodeError:
            self.log("AIåˆ†æç»“æœè§£æå¤±è´¥", "warning")
        except Exception as e:
            self.log(f"AIåˆ†æå¼‚å¸¸: {e}", "error")
        
        # å¦‚æœAIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åˆ¤æ–­
        return self._rule_based_analysis(content, has_high_intent)
    
    def _rule_based_analysis(self, content: str, has_high_intent: bool) -> Dict[str, Any]:
        """
        åŸºäºè§„åˆ™çš„ç®€å•åˆ†æï¼ˆAIå¤±è´¥æ—¶çš„å¤‡é€‰ï¼‰
        """
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¹¿å‘Š
        is_ad = any(kw in content for kw in self.AD_FILTER_KEYWORDS)
        if is_ad:
            return {"is_lead": False, "reason": "ç–‘ä¼¼å¹¿å‘Šå†…å®¹"}
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«éœ€æ±‚å…³é”®è¯
        need_keywords = ["æ‰¾", "æ±‚", "æƒ³", "è¦", "éœ€è¦", "æ¨è", "å“ªå®¶", "æ€ä¹ˆé€‰"]
        has_need = any(kw in content for kw in need_keywords)
        
        if has_need:
            return {
                "is_lead": True,
                "confidence": 60 if has_high_intent else 40,
                "intent_level": "high" if has_high_intent else "medium",
                "needs": [],
                "contact_info": {},
                "summary": "è§„åˆ™åŒ¹é…çš„æ½œåœ¨çº¿ç´¢",
                "follow_up_suggestion": "å»ºè®®è¿›ä¸€æ­¥åˆ†æ"
            }
        
        return {"is_lead": False, "reason": "æœªåŒ¹é…åˆ°éœ€æ±‚å…³é”®è¯"}
    
    def _quick_filter(self, content: str) -> bool:
        """
        å¿«é€Ÿè¿‡æ»¤æ˜æ˜¾ä¸æ˜¯çº¿ç´¢çš„å†…å®¹
        """
        # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
        if len(content) < 15:
            return True
        
        # è¿‡æ»¤æ˜æ˜¾çš„å¹¿å‘Š
        ad_strong_keywords = [
            "æ‹›å•†åŠ ç›Ÿ", "ä»£ç†å•†æ‹›å‹Ÿ", "è¯šæ‹›ä»£ç†",
            "æˆ‘å¸ä¸“ä¸š", "æœ¬å…¬å¸ä¸“ä¸š", "æ¬¢è¿æ¥ç”µ",
            "ä¸šåŠ¡åˆä½œ", "æ‹›è˜å¸æœº", "æ‹›è˜ä¸šåŠ¡å‘˜"
        ]
        for kw in ad_strong_keywords:
            if kw in content:
                return True
        
        return False
    
    def _extract_contact_info(self, content: str) -> Dict[str, str]:
        """
        ä»å†…å®¹ä¸­æå–è”ç³»æ–¹å¼
        """
        contact = {
            "phone": "",
            "email": "",
            "wechat": "",
            "qq": "",
            "name": "",
            "company": ""
        }
        
        # æå–æ‰‹æœºå·
        phone_pattern = r'1[3-9]\d{9}'
        phones = re.findall(phone_pattern, content)
        if phones:
            contact["phone"] = phones[0]
        
        # æå–é‚®ç®±
        email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
        emails = re.findall(email_pattern, content)
        if emails:
            contact["email"] = emails[0]
        
        # æå–å¾®ä¿¡å·
        wechat_patterns = [
            r'å¾®ä¿¡[ï¼š:]\s*([a-zA-Z0-9_-]+)',
            r'wx[ï¼š:]\s*([a-zA-Z0-9_-]+)',
            r'V[ï¼š:]\s*([a-zA-Z0-9_-]+)',
            r'WeChat[ï¼š:]\s*([a-zA-Z0-9_-]+)'
        ]
        for pattern in wechat_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                contact["wechat"] = match.group(1)
                break
        
        # æå–QQ
        qq_patterns = [
            r'QQ[ï¼š:]\s*(\d{5,12})',
            r'qq[ï¼š:]\s*(\d{5,12})'
        ]
        for pattern in qq_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                contact["qq"] = match.group(1)
                break
        
        return contact
    
    async def _search_leads(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ¹æ®æŒ‡å®šå‚æ•°æœç´¢çº¿ç´¢
        """
        keywords = input_data.get("keywords", self.FALLBACK_KEYWORDS[:3])
        sources = input_data.get("sources", ["google"])
        
        results = []
        for keyword in keywords:
            for source in sources:
                site_filter = ""
                if source == "weibo":
                    site_filter = "site:weibo.com"
                elif source == "zhihu":
                    site_filter = "site:zhihu.com"
                elif source == "tieba":
                    site_filter = "site:tieba.baidu.com"
                
                query = f"{keyword} {site_filter}".strip()
                search_results = await self._search_with_serper(query)
                results.extend(search_results)
        
        return {"results": results, "count": len(results)}
    
    async def add_keyword(self, keyword: str, keyword_type: str = "general", 
                          platform: str = None, priority: int = 5) -> Dict[str, Any]:
        """
        æ·»åŠ æ–°çš„æœç´¢å…³é”®è¯
        """
        try:
            from app.models.database import async_session_maker
            from sqlalchemy import text
            
            async with async_session_maker() as db:
                await db.execute(
                    text("""
                        INSERT INTO lead_hunt_keywords 
                        (keyword, keyword_type, platform, priority)
                        VALUES (:keyword, :type, :platform, :priority)
                        ON CONFLICT (keyword, platform) DO UPDATE SET
                            priority = :priority,
                            is_active = true,
                            updated_at = NOW()
                    """),
                    {"keyword": keyword, "type": keyword_type, 
                     "platform": platform, "priority": priority}
                )
                await db.commit()
            
            return {"success": True, "keyword": keyword}
        except Exception as e:
            self.log(f"æ·»åŠ å…³é”®è¯å¤±è´¥: {e}", "error")
            return {"success": False, "error": str(e)}


    # ==================== è¯é¢˜å‘ç°æ¨¡å¼ï¼ˆæ–°å¢ï¼‰====================
    
    def _enhance_search_keyword(self, keyword: str) -> str:
        """
        å¢å¼ºæœç´¢å…³é”®è¯ï¼Œæ·»åŠ åŒä¹‰è¯å’Œç›¸å…³æœ¯è¯­ä»¥æé«˜æœç´¢è¦†ç›–ç‡
        """
        # å…³é”®è¯åŒä¹‰è¯æ˜ å°„
        keyword_synonyms = {
            "è·¨å¢ƒç”µå•†ç‰©æµ": "è·¨å¢ƒç”µå•†ç‰©æµ OR è·¨å¢ƒç‰©æµ OR å›½é™…ç”µå•†ç‰©æµ",
            "å›½é™…è´§è¿ä»£ç†": "å›½é™…è´§è¿ä»£ç† OR è´§ä»£ OR å›½é™…è´§ä»£",
            "æµ·å¤–ä»“": "æµ·å¤–ä»“ OR æµ·å¤–ä»“å‚¨ OR å¢ƒå¤–ä»“",
            "åŒæ¸…åŒ…ç¨": "åŒæ¸…åŒ…ç¨ OR DDP OR åŒæ¸…",
            "æµ·è¿è´¹æŸ¥è¯¢": "æµ·è¿è´¹æŸ¥è¯¢ OR æµ·è¿ä»·æ ¼ OR æµ·è¿è´¹ç”¨",
            "FBAå¤´ç¨‹": "FBAå¤´ç¨‹ OR FBAç‰©æµ OR äºšé©¬é€Šå¤´ç¨‹",
            "æ¸…å…³": "æ¸…å…³ OR æŠ¥å…³ OR é€šå…³",
        }
        
        # å¦‚æœå…³é”®è¯æœ‰åŒä¹‰è¯ï¼Œä½¿ç”¨æ‰©å±•åçš„æŸ¥è¯¢
        if keyword in keyword_synonyms:
            return keyword_synonyms[keyword]
        
        # å¯¹äºåŒ…å«ç‰¹å®šæœ¯è¯­çš„å…³é”®è¯ï¼Œæ·»åŠ ç›¸å…³æœç´¢è¯
        if "ç‰©æµ" in keyword and "è·¨å¢ƒ" not in keyword:
            return f"{keyword} OR å›½é™…ç‰©æµ"
        if "è´§ä»£" in keyword:
            return f"{keyword} OR å›½é™…è´§ä»£"
        if "FBA" in keyword:
            return f"{keyword} OR äºšé©¬é€Šç‰©æµ"
        
        return keyword

    async def _discover_topics(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å‘ç°çƒ­é—¨è¯é¢˜ - ç”¨äºå†…å®¹å¼•æµ
        æœç´¢çŸ¥ä¹ã€å°çº¢ä¹¦ç­‰å¹³å°çš„çƒ­é—¨ç‰©æµç›¸å…³è¯é¢˜
        """
        self.log("ğŸ” å¼€å§‹å‘ç°çƒ­é—¨è¯é¢˜...")
        start_time = datetime.now()
        
        # æ£€æŸ¥APIé…ç½®
        api_key = getattr(settings, 'SERPER_API_KEY', None)
        if not api_key:
            return {
                "error": "æœç´¢APIæœªé…ç½®",
                "message": "è¯·åœ¨ç³»ç»Ÿè®¾ç½®ä¸­é…ç½® SERPER_API_KEY"
            }
        
        results = {
            "discover_time": datetime.now().isoformat(),
            "mode": "topic_discovery",
            "topics_found": [],
            "total_topics": 0,
            "high_value_topics": 0,
            "platforms_searched": [],
            "keywords_used": []
        }
        
        try:
            from app.models.database import async_session_maker
            from sqlalchemy import text
            
            async with async_session_maker() as db:
                # 1. è·å–æœç´¢å…³é”®è¯
                max_keywords = input_data.get("max_keywords", 8)
                
                kw_result = await db.execute(
                    text("""
                        SELECT id, keyword, category, platform, priority
                        FROM topic_search_keywords
                        WHERE is_active = true
                        ORDER BY priority DESC, RANDOM()
                        LIMIT :limit
                    """),
                    {"limit": max_keywords}
                )
                keywords_data = kw_result.fetchall()
                
                if not keywords_data:
                    # ä½¿ç”¨é»˜è®¤å…³é”®è¯
                    default_keywords = [
                        ("FBAå¤´ç¨‹è´¹ç”¨", "æŠ¥ä»·å’¨è¯¢"),
                        ("è´§ç‰©è¢«æ‰£æ€ä¹ˆåŠ", "é—®é¢˜æ±‚åŠ©"),
                        ("è´§ä»£æ€ä¹ˆé€‰", "é€‰æ‹©å’¨è¯¢"),
                        ("æµ·è¿æ¸…å…³æµç¨‹", "æµç¨‹å’¨è¯¢"),
                        ("å›½é™…ç‰©æµæŠ¥ä»·", "æŠ¥ä»·å’¨è¯¢"),
                        ("è·¨å¢ƒç”µå•†ç‰©æµ", "è¡Œä¸šè®¨è®º"),
                        ("å›½é™…è´§è¿ä»£ç†", "é€‰æ‹©å’¨è¯¢"),
                        ("æµ·å¤–ä»“æœåŠ¡", "æµç¨‹å’¨è¯¢"),
                        ("åŒæ¸…åŒ…ç¨", "æµç¨‹å’¨è¯¢"),
                        ("æµ·è¿è´¹æŸ¥è¯¢", "æŠ¥ä»·å’¨è¯¢")
                    ]
                    keywords_data = [(None, kw, cat, None, 8) for kw, cat in default_keywords]
                
                self.log(f"ä½¿ç”¨ {len(keywords_data)} ä¸ªå…³é”®è¯æœç´¢è¯é¢˜")
                
                # 2. å®šä¹‰æœç´¢å¹³å°ï¼ˆæ‰©å±•æ›´å¤šå¹³å°ï¼Œæé«˜è¯é¢˜å‘ç°è¦†ç›–ç‡ï¼‰
                platforms = [
                    ("zhihu", "site:zhihu.com/question", "çŸ¥ä¹é—®ç­”"),
                    ("xiaohongshu", "site:xiaohongshu.com", "å°çº¢ä¹¦"),
                    ("baidu_zhidao", "site:zhidao.baidu.com", "ç™¾åº¦çŸ¥é“"),
                    ("tieba", "site:tieba.baidu.com", "ç™¾åº¦è´´å§"),
                    ("douyin", "site:douyin.com", "æŠ–éŸ³"),
                    ("weibo", "site:weibo.com", "å¾®åš"),
                ]
                
                all_topics = []
                
                # 3. å¯¹æ¯ä¸ªå…³é”®è¯åœ¨æ¯ä¸ªå¹³å°æœç´¢
                for kw_data in keywords_data:
                    kw_id, keyword, category, kw_platform, priority = kw_data
                    results["keywords_used"].append(keyword)
                    
                    # å¢å¼ºæœç´¢å…³é”®è¯
                    enhanced_keyword = self._enhance_search_keyword(keyword)
                    
                    for platform_id, site_filter, platform_name in platforms:
                        # å¦‚æœå…³é”®è¯æŒ‡å®šäº†å¹³å°ï¼Œåªæœç´¢è¯¥å¹³å°
                        if kw_platform and kw_platform != platform_id:
                            continue
                        
                        try:
                            # æ„å»ºæœç´¢æŸ¥è¯¢ï¼ˆä¼˜åŒ–æŸ¥è¯¢é€»è¾‘ï¼Œæ”¯æŒæ›´å¹¿æ³›çš„è¡Œä¸šæœ¯è¯­ï¼‰
                            query = f"{enhanced_keyword} {site_filter}".strip()
                            self.log(f"ğŸ” æœç´¢: {query}")
                            
                            # è¯é¢˜å‘ç°ä¸¥æ ¼é™åˆ¶åœ¨è¿‡å»ä¸€ä¸ªæœˆå†…ï¼Œç¡®ä¿å†…å®¹çš„æ—¶æ•ˆæ€§
                            search_results = await self._search_with_serper(query, time_range="m")
                            
                            if search_results:
                                self.log(f"âœ… {platform_name} è¿”å› {len(search_results)} æ¡ç»“æœ")
                                results["platforms_searched"].append(platform_name)
                                
                                for item in search_results[:5]:  # æ¯ä¸ªå…³é”®è¯æ¯ä¸ªå¹³å°å–å‰5æ¡
                                    url = item.get("url", "")
                                    title = item.get("title", "")
                                    
                                    if not url or not title:
                                        continue
                                    
                                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                                    url_hash = hashlib.md5(url.encode()).hexdigest()
                                    
                                    existing = await db.execute(
                                        text("SELECT id FROM hot_topics WHERE url_hash = :hash"),
                                        {"hash": url_hash}
                                    )
                                    if existing.fetchone():
                                        continue
                                    
                                    # åˆ†æè¯é¢˜ä»·å€¼
                                    self.log(f"ğŸ§  AIåˆ†æè¯é¢˜ä»·å€¼: {title[:30]}...")
                                    topic_analysis = await self._analyze_topic_value({
                                        "title": title,
                                        "content": item.get("content", ""),
                                        "url": url,
                                        "platform": platform_id,
                                        "category": category
                                    })
                                    
                                    if topic_analysis.get("is_valuable", False):
                                        self.log(f"ğŸ¯ å‘ç°é«˜ä»·å€¼è¯é¢˜: {title[:30]} (åˆ†æ•°: {topic_analysis.get('value_score')})")
                                        
                                        # ç«‹å³ä¿å­˜è¯é¢˜åˆ°æ•°æ®åº“ï¼Œå®ç°å¢é‡æ›´æ–°
                                        await db.execute(
                                            text("""
                                                INSERT INTO hot_topics 
                                                (title, url, url_hash, platform, category, keywords,
                                                 value_score, ai_summary, ai_answer_strategy, 
                                                 ai_recommended_points, priority, status)
                                                VALUES 
                                                (:title, :url, :url_hash, :platform, :category, :keywords,
                                                 :value_score, :summary, :strategy, :points, :priority, 'new')
                                                ON CONFLICT (url_hash) DO NOTHING
                                            """),
                                            {
                                                "title": title,
                                                "url": url,
                                                "url_hash": url_hash,
                                                "platform": platform_id,
                                                "category": category,
                                                "keywords": [keyword],
                                                "value_score": topic_analysis.get("value_score", 50),
                                                "summary": topic_analysis.get("summary", ""),
                                                "strategy": topic_analysis.get("answer_strategy", ""),
                                                "points": topic_analysis.get("recommended_points", []),
                                                "priority": "high" if topic_analysis.get("value_score", 0) >= 70 else "medium"
                                            }
                                        )
                                        await db.commit()  # ç«‹å³æäº¤
                                        
                                        topic_data = {
                                            "title": title,
                                            "url": url,
                                            "url_hash": url_hash,
                                            "platform": platform_id,
                                            "category": category,
                                            "keyword": keyword,
                                            "value_score": topic_analysis.get("value_score", 50)
                                        }
                                        results["topics_found"].append(topic_data)
                                        results["total_topics"] += 1
                                        if topic_analysis.get("value_score", 0) >= 70:
                                            results["high_value_topics"] += 1
                                    else:
                                        self.log(f"â­ï¸ è¯é¢˜ä»·å€¼ä¸è¶³ï¼Œè·³è¿‡: {title[:30]} (ç†ç”±: {topic_analysis.get('reason')})")
                            else:
                                self.log(f"âŒ {platform_name} æœªè¿”å›ç»“æœ")
                            
                            # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                            await asyncio.sleep(0.5)
                            
                        except Exception as e:
                            self.log(f"æœç´¢è¯é¢˜å¤±è´¥ ({platform_name}, {keyword}): {e}", "error")
                
                # 4. æ›´æ–°å°çŒçš„ä»»åŠ¡ç»Ÿè®¡
                await db.execute(
                    text("""
                        UPDATE ai_agents
                        SET tasks_completed_today = tasks_completed_today + 1,
                            tasks_completed_total = tasks_completed_total + 1,
                            last_active_at = NOW(),
                            updated_at = NOW()
                        WHERE agent_type = 'lead_hunter'
                    """)
                )
                await db.commit()
                
        except Exception as e:
            self.log(f"è¯é¢˜å‘ç°å‡ºé”™: {e}", "error")
            results["error"] = str(e)
        
        # å»é‡å¹³å°åˆ—è¡¨
        results["platforms_searched"] = list(set(results["platforms_searched"]))
        
        duration = (datetime.now() - start_time).total_seconds()
        results["duration_seconds"] = round(duration, 2)
        
        self.log(f"âœ… è¯é¢˜å‘ç°å®Œæˆï¼è€—æ—¶{duration:.1f}ç§’ï¼Œå‘ç° {results['total_topics']} ä¸ªè¯é¢˜ï¼Œ"
                 f"é«˜ä»·å€¼ {results['high_value_topics']} ä¸ª")
        
        return results
    
    async def _analyze_topic_value(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ†æè¯é¢˜ä»·å€¼ï¼Œåˆ¤æ–­æ˜¯å¦å€¼å¾—å›ç­”
        """
        title = input_data.get("title", "")
        content = input_data.get("content", "")
        url = input_data.get("url", "")
        platform = input_data.get("platform", "")
        category = input_data.get("category", "")
        
        if not title:
            return {"is_valuable": False, "reason": "æ ‡é¢˜ä¸ºç©º"}
        
        # å¿«é€Ÿè¿‡æ»¤å¹¿å‘Šå’Œæ— æ•ˆå†…å®¹
        ad_keywords = ["å¹¿å‘Š", "æ¨å¹¿", "ä¼˜æƒ ", "é™æ—¶", "åŠ ç›Ÿ", "æ‹›å•†", "ä»£ç†"]
        if any(kw in title for kw in ad_keywords):
            return {"is_valuable": False, "reason": "ç–‘ä¼¼å¹¿å‘Š"}
        
        # ä½¿ç”¨AIåˆ†æè¯é¢˜ä»·å€¼
        prompt = f"""è¯·åˆ†æä»¥ä¸‹è¯é¢˜æ˜¯å¦å€¼å¾—ä¸€ä¸ªç‰©æµ/è´§ä»£å…¬å¸å»å›ç­”ï¼ˆå†…å®¹å¼•æµç›®çš„ï¼‰ï¼š

å¹³å°ï¼š{platform}
æ ‡é¢˜ï¼š{title}
å†…å®¹æ‘˜è¦ï¼š{content[:300] if content else 'æ— '}

è¯·ä»ä»¥ä¸‹è§’åº¦åˆ†æï¼š
1. è¿™ä¸ªè¯é¢˜æ˜¯å¦ä¸å›½é™…ç‰©æµ/è´§ä»£æœåŠ¡ç›¸å…³ï¼Ÿ
2. æé—®è€…æ˜¯å¦å¯èƒ½æ˜¯æ½œåœ¨å®¢æˆ·ï¼Ÿ
3. å›ç­”è¿™ä¸ªé—®é¢˜èƒ½å¦å±•ç¤ºä¸“ä¸šæ€§ï¼Ÿ
4. é¢„è®¡èƒ½å¸¦æ¥å¤šå°‘æ›å…‰ï¼Ÿ

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "is_valuable": true/false,
    "value_score": 0-100,
    "summary": "è¯é¢˜æ ¸å¿ƒæ˜¯ä»€ä¹ˆ",
    "answer_strategy": "å»ºè®®å¦‚ä½•å›ç­”è¿™ä¸ªé—®é¢˜",
    "recommended_points": ["å›ç­”è¦ç‚¹1", "å›ç­”è¦ç‚¹2", "å›ç­”è¦ç‚¹3"],
    "potential_exposure": "high/medium/low",
    "reason": "åˆ¤æ–­ç†ç”±"
}}"""
        
        try:
            response = await self.think([{"role": "user", "content": prompt}])
            
            # è§£æAIå›å¤
            json_start = response.find("{")
            json_end = response.rfind("}") + 1
            if json_start != -1 and json_end > json_start:
                result = json.loads(response[json_start:json_end])
                return result
        except Exception as e:
            self.log(f"AIåˆ†æè¯é¢˜å¤±è´¥: {e}", "warning")
        
        # å¦‚æœAIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åˆ¤æ–­
        value_keywords = ["æ€ä¹ˆ", "å¦‚ä½•", "æ¨è", "å“ªå®¶", "å¤šå°‘é’±", "è´¹ç”¨", "æµç¨‹", "é—®é¢˜"]
        has_value = any(kw in title for kw in value_keywords)
        
        return {
            "is_valuable": has_value,
            "value_score": 60 if has_value else 30,
            "summary": title[:50],
            "answer_strategy": "æä¾›ä¸“ä¸šå»ºè®®ï¼Œå±•ç¤ºå…¬å¸ä¼˜åŠ¿",
            "recommended_points": ["ä¸“ä¸šè§£ç­”", "æ¡ˆä¾‹åˆ†äº«", "è”ç³»æ–¹å¼"],
            "reason": "è§„åˆ™åˆ¤æ–­"
        }
    
    async def _generate_answer(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä¸ºè¯é¢˜ç”Ÿæˆä¸“ä¸šå›ç­”å†…å®¹ï¼ˆè°ƒç”¨å°æ–‡ï¼‰
        """
        topic_id = input_data.get("topic_id")
        
        if not topic_id:
            return {"error": "ç¼ºå°‘è¯é¢˜ID"}
        
        try:
            from app.models.database import async_session_maker
            from sqlalchemy import text
            from app.agents.copywriter import copywriter_agent
            
            async with async_session_maker() as db:
                # è·å–è¯é¢˜ä¿¡æ¯
                result = await db.execute(
                    text("""
                        SELECT title, url, platform, category, 
                               ai_summary, ai_answer_strategy, ai_recommended_points
                        FROM hot_topics WHERE id = :id
                    """),
                    {"id": topic_id}
                )
                topic = result.fetchone()
                
                if not topic:
                    return {"error": "è¯é¢˜ä¸å­˜åœ¨"}
                
                title, url, platform, category, summary, strategy, points = topic
                
                # è·å–å…¬å¸ä¿¡æ¯
                company_result = await db.execute(
                    text("SELECT company_name, company_intro, advantages, contact_wechat, contact_phone FROM company_config LIMIT 1")
                )
                company = company_result.fetchone()
                
                company_name = company[0] if company else "æˆ‘ä»¬å…¬å¸"
                company_intro = company[1] if company else ""
                advantages = company[2] if company else []
                contact_wechat = company[3] if company else ""
                contact_phone = company[4] if company else ""
                
                # è°ƒç”¨å°æ–‡ç”Ÿæˆå†…å®¹
                content_result = await copywriter_agent.process({
                    "action": "generate",
                    "content_type": "zhihu_answer" if platform == "zhihu" else "social_post",
                    "topic": title,
                    "context": {
                        "platform": platform,
                        "category": category,
                        "summary": summary,
                        "strategy": strategy,
                        "recommended_points": points,
                        "company_name": company_name,
                        "company_intro": company_intro,
                        "advantages": advantages,
                        "contact_wechat": contact_wechat,
                        "contact_phone": contact_phone
                    }
                })
                
                generated_content = content_result.get("content", "")
                
                if generated_content:
                    # ä¿å­˜ç”Ÿæˆçš„å†…å®¹
                    await db.execute(
                        text("""
                            UPDATE hot_topics 
                            SET generated_content = :content,
                                generated_at = NOW(),
                                updated_at = NOW()
                            WHERE id = :id
                        """),
                        {"content": generated_content, "id": topic_id}
                    )
                    await db.commit()
                
                return {
                    "success": True,
                    "topic_id": topic_id,
                    "title": title,
                    "platform": platform,
                    "generated_content": generated_content,
                    "url": url
                }
                
        except Exception as e:
            self.log(f"ç”Ÿæˆå›ç­”å¤±è´¥: {e}", "error")
            return {"error": str(e)}
    
    async def _get_topic_stats(self) -> Dict[str, Any]:
        """è·å–è¯é¢˜å‘ç°ç»Ÿè®¡"""
        try:
            from app.models.database import async_session_maker
            from sqlalchemy import text
            
            async with async_session_maker() as db:
                # æ€»è¯é¢˜æ•°
                total_result = await db.execute(
                    text("SELECT COUNT(*) FROM hot_topics")
                )
                total = total_result.scalar() or 0
                
                # å¾…å›ç­”è¯é¢˜æ•°
                new_result = await db.execute(
                    text("SELECT COUNT(*) FROM hot_topics WHERE status = 'new'")
                )
                new_count = new_result.scalar() or 0
                
                # å·²å›ç­”æ•°
                answered_result = await db.execute(
                    text("SELECT COUNT(*) FROM hot_topics WHERE status = 'answered'")
                )
                answered_count = answered_result.scalar() or 0
                
                # é«˜ä»·å€¼è¯é¢˜æ•°
                high_value_result = await db.execute(
                    text("SELECT COUNT(*) FROM hot_topics WHERE value_score >= 70 AND status = 'new'")
                )
                high_value_count = high_value_result.scalar() or 0
                
                # æŒ‰å¹³å°ç»Ÿè®¡
                platform_result = await db.execute(
                    text("""
                        SELECT platform, COUNT(*) 
                        FROM hot_topics 
                        WHERE status = 'new'
                        GROUP BY platform
                    """)
                )
                by_platform = {row[0]: row[1] for row in platform_result.fetchall()}
                
                return {
                    "total": total,
                    "new": new_count,
                    "answered": answered_count,
                    "high_value": high_value_count,
                    "by_platform": by_platform
                }
                
        except Exception as e:
            self.log(f"è·å–è¯é¢˜ç»Ÿè®¡å¤±è´¥: {e}", "error")
            return {"error": str(e)}

    # ==================== äº§å“è¶‹åŠ¿å‘ç°æ¨¡å¼ï¼ˆæ–°å¢ï¼‰====================
    
    async def _discover_product_trends(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å‘ç°æ¬§æ´²è·¨å¢ƒç”µå•†çƒ­é—¨äº§å“è¶‹åŠ¿
        æœç´¢å®Œæˆåäº¤ç»™å°è°ƒå¤„ç†ï¼šå­˜å…¥çŸ¥è¯†åº“ + å‘é€é‚®ä»¶
        """
        self.log("ğŸ›’ å¼€å§‹å‘ç°æ¬§æ´²çƒ­é—¨äº§å“è¶‹åŠ¿...")
        start_time = datetime.now()
        
        # æ£€æŸ¥APIé…ç½®
        api_key = getattr(settings, 'SERPER_API_KEY', None)
        if not api_key:
            return {
                "error": "æœç´¢APIæœªé…ç½®",
                "message": "è¯·åœ¨ç³»ç»Ÿè®¾ç½®ä¸­é…ç½® SERPER_API_KEY"
            }
        
        results = {
            "discover_time": datetime.now().isoformat(),
            "mode": "product_trend_discovery",
            "products_found": [],
            "total_products": 0,
            "high_trend_products": 0,
            "platforms_searched": [],
            "keywords_used": []
        }
        
        try:
            from app.models.database import async_session_maker
            from sqlalchemy import text
            
            async with async_session_maker() as db:
                # 1. è·å–æœç´¢å…³é”®è¯
                max_keywords = input_data.get("max_keywords", 10)
                
                kw_result = await db.execute(
                    text("""
                        SELECT id, keyword, category, platform, priority
                        FROM product_trend_keywords
                        WHERE is_active = true
                        ORDER BY priority DESC, RANDOM()
                        LIMIT :limit
                    """),
                    {"limit": max_keywords}
                )
                keywords_data = kw_result.fetchall()
                
                if not keywords_data:
                    # ä½¿ç”¨é»˜è®¤å…³é”®è¯
                    default_keywords = [
                        ("æ¬§æ´²è·¨å¢ƒç”µå•†çˆ†æ¬¾ 2026", "ç»¼åˆ"),
                        ("å¾·å›½äºšé©¬é€Šçƒ­é”€äº§å“", "äºšé©¬é€Š"),
                        ("è‹±å›½ç”µå•†çƒ­å–", "ç»¼åˆ"),
                        ("Temuæ¬§æ´²çƒ­é”€", "æ–°å¹³å°"),
                        ("æ¬§æ´²å®¶å±…ç”¨å“çƒ­é”€", "å®¶å±…"),
                    ]
                    keywords_data = [(None, kw, cat, None, 8) for kw, cat in default_keywords]
                
                self.log(f"ä½¿ç”¨ {len(keywords_data)} ä¸ªå…³é”®è¯æœç´¢äº§å“è¶‹åŠ¿")
                
                # 2. å®šä¹‰æœç´¢å¹³å°
                platforms = [
                    ("google", "", "è°·æ­Œæœç´¢"),
                    ("baidu", "site:baidu.com", "ç™¾åº¦"),
                ]
                
                all_products = []
                
                # 3. å¯¹æ¯ä¸ªå…³é”®è¯æœç´¢
                for kw_data in keywords_data:
                    kw_id, keyword, category, kw_platform, priority = kw_data
                    results["keywords_used"].append(keyword)
                    
                    for platform_id, site_filter, platform_name in platforms:
                        try:
                            # æ„å»ºæœç´¢æŸ¥è¯¢
                            query = f"{keyword} {site_filter}".strip()
                            self.log(f"ğŸ” æœç´¢: {query}")
                            
                            search_results = await self._search_with_serper(query)
                            
                            if search_results:
                                results["platforms_searched"].append(platform_name)
                                
                                for item in search_results[:5]:  # æ¯ä¸ªå…³é”®è¯å–å‰5æ¡
                                    url = item.get("url", "")
                                    title = item.get("title", "")
                                    content = item.get("content", "")
                                    
                                    if not url or not title:
                                        continue
                                    
                                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                                    existing = await db.execute(
                                        text("SELECT id FROM product_trends WHERE source_url = :url"),
                                        {"url": url}
                                    )
                                    if existing.fetchone():
                                        continue
                                    
                                    # AIåˆ†æäº§å“è¶‹åŠ¿
                                    product_analysis = await self._analyze_product_trend({
                                        "title": title,
                                        "content": content,
                                        "url": url,
                                        "platform": platform_id,
                                        "category": category,
                                        "keyword": keyword
                                    })
                                    
                                    if product_analysis.get("is_valid_product", False):
                                        product_data = {
                                            "product_name": product_analysis.get("product_name", title[:100]),
                                            "category": product_analysis.get("category", category),
                                            "description": product_analysis.get("description", content[:500]),
                                            "source_url": url,
                                            "source_platform": platform_id,
                                            "source_region": "europe",
                                            "sales_volume": product_analysis.get("sales_volume", ""),
                                            "price_range": product_analysis.get("price_range", ""),
                                            "growth_rate": product_analysis.get("growth_rate", ""),
                                            "trend_score": product_analysis.get("trend_score", 50),
                                            "ai_analysis": product_analysis.get("analysis", ""),
                                            "ai_opportunity": product_analysis.get("opportunity", ""),
                                            "ai_logistics_tips": product_analysis.get("logistics_tips", ""),
                                            "keywords": [keyword] + product_analysis.get("keywords", [])
                                        }
                                        all_products.append(product_data)
                            
                            # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                            await asyncio.sleep(0.5)
                            
                        except Exception as e:
                            self.log(f"æœç´¢äº§å“è¶‹åŠ¿å¤±è´¥ ({platform_name}, {keyword}): {e}", "error")
                
                # 4. ä¿å­˜äº§å“è¶‹åŠ¿åˆ°æ•°æ®åº“
                saved_products = []
                for product in all_products:
                    try:
                        result = await db.execute(
                            text("""
                                INSERT INTO product_trends 
                                (product_name, category, description, source_url, source_platform,
                                 source_region, sales_volume, price_range, growth_rate, trend_score,
                                 ai_analysis, ai_opportunity, ai_logistics_tips, keywords, status)
                                VALUES 
                                (:name, :category, :desc, :url, :platform, :region, :sales,
                                 :price, :growth, :score, :analysis, :opportunity, :logistics, :keywords, 'new')
                                ON CONFLICT DO NOTHING
                                RETURNING id
                            """),
                            {
                                "name": product["product_name"],
                                "category": product["category"],
                                "desc": product["description"],
                                "url": product["source_url"],
                                "platform": product["source_platform"],
                                "region": product["source_region"],
                                "sales": product["sales_volume"],
                                "price": product["price_range"],
                                "growth": product["growth_rate"],
                                "score": product["trend_score"],
                                "analysis": product["ai_analysis"],
                                "opportunity": product["ai_opportunity"],
                                "logistics": product["ai_logistics_tips"],
                                "keywords": product["keywords"]
                            }
                        )
                        row = result.fetchone()
                        if row:
                            product["id"] = str(row[0])
                            saved_products.append(product)
                            results["products_found"].append(product)
                            results["total_products"] += 1
                            if product["trend_score"] >= 70:
                                results["high_trend_products"] += 1
                            
                    except Exception as e:
                        self.log(f"ä¿å­˜äº§å“è¶‹åŠ¿å¤±è´¥: {e}", "error")
                
                await db.commit()
                
                # 5. æ›´æ–°å°çŒçš„ä»»åŠ¡ç»Ÿè®¡
                await db.execute(
                    text("""
                        UPDATE ai_agents
                        SET tasks_completed_today = tasks_completed_today + 1,
                            tasks_completed_total = tasks_completed_total + 1,
                            last_active_at = NOW(),
                            updated_at = NOW()
                        WHERE agent_type = 'lead_hunter'
                    """)
                )
                await db.commit()
                
                # 6. å¦‚æœæœ‰å‘ç°äº§å“ï¼Œäº¤ç»™å°è°ƒå¤„ç†
                if saved_products:
                    await self._notify_coordinator_for_products(saved_products, db)
                
        except Exception as e:
            self.log(f"äº§å“è¶‹åŠ¿å‘ç°å‡ºé”™: {e}", "error")
            results["error"] = str(e)
        
        # å»é‡å¹³å°åˆ—è¡¨
        results["platforms_searched"] = list(set(results["platforms_searched"]))
        
        duration = (datetime.now() - start_time).total_seconds()
        results["duration_seconds"] = round(duration, 2)
        
        self.log(f"âœ… äº§å“è¶‹åŠ¿å‘ç°å®Œæˆï¼è€—æ—¶{duration:.1f}ç§’ï¼Œå‘ç° {results['total_products']} ä¸ªäº§å“ï¼Œ"
                 f"é«˜è¶‹åŠ¿ {results['high_trend_products']} ä¸ª")
        
        return results
    
    async def _analyze_product_trend(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ†æäº§å“è¶‹åŠ¿ä»·å€¼
        """
        title = input_data.get("title", "")
        content = input_data.get("content", "")
        url = input_data.get("url", "")
        category = input_data.get("category", "")
        keyword = input_data.get("keyword", "")
        
        if not title:
            return {"is_valid_product": False, "reason": "æ ‡é¢˜ä¸ºç©º"}
        
        # å¿«é€Ÿè¿‡æ»¤éäº§å“å†…å®¹
        invalid_keywords = ["æ‹›è˜", "åŠ ç›Ÿ", "ä»£ç†", "åŸ¹è®­", "è¯¾ç¨‹", "æ•™ç¨‹"]
        if any(kw in title for kw in invalid_keywords):
            return {"is_valid_product": False, "reason": "éäº§å“å†…å®¹"}
        
        # ä½¿ç”¨AIåˆ†æäº§å“è¶‹åŠ¿
        prompt = f"""è¯·åˆ†æä»¥ä¸‹æœç´¢ç»“æœæ˜¯å¦æ˜¯æœ‰ä»·å€¼çš„æ¬§æ´²è·¨å¢ƒç”µå•†äº§å“è¶‹åŠ¿ä¿¡æ¯ï¼š

æœç´¢å…³é”®è¯ï¼š{keyword}
æ ‡é¢˜ï¼š{title}
å†…å®¹æ‘˜è¦ï¼š{content[:500] if content else 'æ— '}
URLï¼š{url}

è¯·ä»ä»¥ä¸‹è§’åº¦åˆ†æï¼š
1. è¿™æ˜¯å¦æ˜¯å…·ä½“çš„äº§å“æˆ–äº§å“ç±»ç›®ä¿¡æ¯ï¼Ÿ
2. è¿™ä¸ªäº§å“åœ¨æ¬§æ´²å¸‚åœºçš„çƒ­åº¦å¦‚ä½•ï¼Ÿ
3. ä½œä¸ºç‰©æµå…¬å¸ï¼Œäº†è§£è¿™ä¸ªä¿¡æ¯æœ‰ä»€ä¹ˆä»·å€¼ï¼Ÿ
4. è¿™ç±»äº§å“çš„ç‰©æµéœ€æ±‚ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "is_valid_product": true/false,
    "product_name": "äº§å“åç§°",
    "category": "äº§å“ç±»åˆ«",
    "description": "äº§å“ç®€è¦æè¿°",
    "sales_volume": "é”€é‡æè¿°ï¼Œå¦‚'çƒ­é”€'ã€'10ä¸‡+'ç­‰",
    "price_range": "ä»·æ ¼åŒºé—´ï¼Œå¦‚'â‚¬10-30'",
    "growth_rate": "å¢é•¿ç‡æè¿°ï¼Œå¦‚'å¢é•¿50%'",
    "trend_score": 0-100,
    "analysis": "äº§å“è¶‹åŠ¿åˆ†ææ‘˜è¦",
    "opportunity": "å¯¹ç‰©æµå…¬å¸çš„å•†æœºåˆ†æ",
    "logistics_tips": "é’ˆå¯¹è¯¥äº§å“çš„ç‰©æµå»ºè®®ï¼ˆåŒ…è£…ã€æ—¶æ•ˆã€æ¸…å…³ç­‰ï¼‰",
    "keywords": ["ç›¸å…³å…³é”®è¯1", "å…³é”®è¯2"],
    "reason": "åˆ¤æ–­ç†ç”±"
}}"""
        
        try:
            response = await self.think([{"role": "user", "content": prompt}])
            
            # è§£æAIå›å¤
            json_start = response.find("{")
            json_end = response.rfind("}") + 1
            if json_start != -1 and json_end > json_start:
                result = json.loads(response[json_start:json_end])
                return result
        except Exception as e:
            self.log(f"AIåˆ†æäº§å“è¶‹åŠ¿å¤±è´¥: {e}", "warning")
        
        # å¦‚æœAIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åˆ¤æ–­
        product_keywords = ["çƒ­é”€", "çˆ†æ¬¾", "ç•…é”€", "çƒ­å–", "é”€é‡", "æ’è¡Œ", "è¶‹åŠ¿"]
        has_product_signal = any(kw in title or kw in content for kw in product_keywords)
        
        return {
            "is_valid_product": has_product_signal,
            "product_name": title[:100],
            "category": category,
            "description": content[:300],
            "trend_score": 50 if has_product_signal else 30,
            "analysis": "è§„åˆ™åˆ¤æ–­",
            "reason": "è§„åˆ™åŒ¹é…"
        }
    
    async def _notify_coordinator_for_products(self, products: List[Dict], db) -> None:
        """
        é€šçŸ¥å°è°ƒå¤„ç†äº§å“è¶‹åŠ¿ä¿¡æ¯
        1. å­˜å…¥çŸ¥è¯†åº“
        2. å‘é€é‚®ä»¶é€šçŸ¥
        """
        try:
            from sqlalchemy import text
            from app.agents.coordinator import coordinator_agent
            from app.services.email_service import email_service
            
            self.log(f"ğŸ“¤ é€šçŸ¥å°è°ƒå¤„ç† {len(products)} ä¸ªäº§å“è¶‹åŠ¿...")
            
            # å‡†å¤‡äº§å“æ‘˜è¦
            product_summary = []
            for p in products:
                summary = f"""
ğŸ›’ **{p.get('product_name', 'æœªçŸ¥äº§å“')}**
- ç±»åˆ«: {p.get('category', 'æœªçŸ¥')}
- è¶‹åŠ¿è¯„åˆ†: {p.get('trend_score', 0)}åˆ†
- é”€é‡: {p.get('sales_volume', 'æœªçŸ¥')}
- ä»·æ ¼: {p.get('price_range', 'æœªçŸ¥')}
- åˆ†æ: {p.get('ai_analysis', 'æš‚æ— ')}
- ç‰©æµå»ºè®®: {p.get('ai_logistics_tips', 'æš‚æ— ')}
- æ¥æº: {p.get('source_url', '')}
"""
                product_summary.append(summary)
            
            # 1. å­˜å…¥çŸ¥è¯†åº“ï¼ˆä½œä¸ºå¸‚åœºæƒ…æŠ¥ï¼‰
            knowledge_content = f"""
# æ¬§æ´²è·¨å¢ƒç”µå•†äº§å“è¶‹åŠ¿æŠ¥å‘Š

å‘ç°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}
å‘ç°æ•°é‡: {len(products)} ä¸ªäº§å“

## äº§å“è¯¦æƒ…

{''.join(product_summary)}

## æ€»ç»“

æœ¬æ¬¡å‘ç°äº† {len(products)} ä¸ªæ¬§æ´²å¸‚åœºçƒ­é—¨äº§å“è¶‹åŠ¿ï¼Œå»ºè®®å…³æ³¨é«˜è¶‹åŠ¿è¯„åˆ†çš„äº§å“ï¼Œ
åŠæ—¶è°ƒæ•´ç‰©æµæœåŠ¡ç­–ç•¥ï¼ŒæŠ“ä½å¸‚åœºæœºä¼šã€‚
"""
            
            try:
                await db.execute(
                    text("""
                        INSERT INTO knowledge_base 
                        (title, content, category, tags, source, created_at)
                        VALUES 
                        (:title, :content, 'market_intelligence', :tags, 'lead_hunter', NOW())
                    """),
                    {
                        "title": f"æ¬§æ´²äº§å“è¶‹åŠ¿æŠ¥å‘Š - {datetime.now().strftime('%Y-%m-%d')}",
                        "content": knowledge_content,
                        "tags": ["æ¬§æ´²å¸‚åœº", "äº§å“è¶‹åŠ¿", "è·¨å¢ƒç”µå•†", "å¸‚åœºæƒ…æŠ¥"]
                    }
                )
                self.log("âœ… äº§å“è¶‹åŠ¿å·²å­˜å…¥çŸ¥è¯†åº“")
            except Exception as e:
                self.log(f"å­˜å…¥çŸ¥è¯†åº“å¤±è´¥: {e}", "warning")
            
            # 2. å‘é€é‚®ä»¶é€šçŸ¥
            email_body = f"""
<html>
<head>
    <style>
        body {{ font-family: Arial, sans-serif; line-height: 1.6; color: #333; }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; }}
        .product {{ background: #f8f9fa; padding: 15px; margin: 10px 0; border-radius: 8px; border-left: 4px solid #667eea; }}
        .score {{ font-size: 24px; font-weight: bold; color: #667eea; }}
        .tips {{ background: #e8f4f8; padding: 10px; border-radius: 5px; }}
        .footer {{ margin-top: 20px; padding-top: 20px; border-top: 1px solid #ddd; color: #666; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ›’ æ¬§æ´²çƒ­é—¨äº§å“è¶‹åŠ¿æŠ¥å‘Š</h1>
        <p>å‘ç°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}</p>
        <p>æœ¬æ¬¡å…±å‘ç° <strong>{len(products)}</strong> ä¸ªçƒ­é—¨äº§å“è¶‹åŠ¿</p>
    </div>
    
    <h2>ğŸ“Š äº§å“è¯¦æƒ…</h2>
"""
            
            for p in products:
                score = p.get('trend_score', 0)
                score_color = '#28a745' if score >= 70 else '#ffc107' if score >= 50 else '#6c757d'
                
                email_body += f"""
    <div class="product">
        <h3>{p.get('product_name', 'æœªçŸ¥äº§å“')}</h3>
        <p><strong>ç±»åˆ«:</strong> {p.get('category', 'æœªçŸ¥')}</p>
        <p><strong>è¶‹åŠ¿è¯„åˆ†:</strong> <span class="score" style="color: {score_color}">{score}åˆ†</span></p>
        <p><strong>é”€é‡æƒ…å†µ:</strong> {p.get('sales_volume', 'æœªçŸ¥')}</p>
        <p><strong>ä»·æ ¼åŒºé—´:</strong> {p.get('price_range', 'æœªçŸ¥')}</p>
        <p><strong>è¶‹åŠ¿åˆ†æ:</strong> {p.get('ai_analysis', 'æš‚æ— ')}</p>
        <div class="tips">
            <strong>ğŸ’¡ ç‰©æµå»ºè®®:</strong> {p.get('ai_logistics_tips', 'æš‚æ— ')}
        </div>
        <p><strong>ğŸ”— æ¥æº:</strong> <a href="{p.get('source_url', '#')}">{p.get('source_url', 'æ— ')}</a></p>
    </div>
"""
            
            email_body += f"""
    <div class="footer">
        <p>æ­¤é‚®ä»¶ç”± <strong>å°çŒ (Lead Hunter AI)</strong> è‡ªåŠ¨å‘é€</p>
        <p>äº§å“è¶‹åŠ¿ä¿¡æ¯å·²åŒæ­¥å­˜å…¥çŸ¥è¯†åº“ï¼Œå¯åœ¨ç³»ç»Ÿä¸­æŸ¥çœ‹å®Œæ•´æŠ¥å‘Š</p>
        <p>å¦‚æœ‰é—®é¢˜è¯·è”ç³»ç³»ç»Ÿç®¡ç†å‘˜</p>
    </div>
</body>
</html>
"""
            
            # å‘é€é‚®ä»¶
            try:
                email_result = await email_service.send_email(
                    to_emails=[getattr(settings, 'BOSS_EMAIL', '18757672416@163.com')],
                    subject=f"ğŸ›’ æ¬§æ´²çƒ­é—¨äº§å“è¶‹åŠ¿æŠ¥å‘Š - {datetime.now().strftime('%Y-%m-%d')} ({len(products)}ä¸ªäº§å“)",
                    html_content=email_body
                )
                
                if email_result.get("status") == "sent":
                    self.log("âœ… äº§å“è¶‹åŠ¿æŠ¥å‘Šé‚®ä»¶å·²å‘é€")
                    
                    # æ›´æ–°äº§å“çŠ¶æ€ä¸ºå·²å‘é€é‚®ä»¶
                    for p in products:
                        if p.get("id"):
                            await db.execute(
                                text("""
                                    UPDATE product_trends 
                                    SET is_email_sent = true, updated_at = NOW()
                                    WHERE id = :id
                                """),
                                {"id": p["id"]}
                            )
                else:
                    self.log(f"å‘é€é‚®ä»¶å¤±è´¥: {email_result.get('error')}", "error")
                    
            except Exception as e:
                self.log(f"å‘é€é‚®ä»¶å¼‚å¸¸: {e}", "error")
            
            await db.commit()
            
        except Exception as e:
            self.log(f"é€šçŸ¥å°è°ƒå¤„ç†å¤±è´¥: {e}", "error")
    
    async def _get_product_stats(self) -> Dict[str, Any]:
        """è·å–äº§å“è¶‹åŠ¿ç»Ÿè®¡"""
        try:
            from app.models.database import async_session_maker
            from sqlalchemy import text
            
            async with async_session_maker() as db:
                # æ€»äº§å“æ•°
                total_result = await db.execute(
                    text("SELECT COUNT(*) FROM product_trends")
                )
                total = total_result.scalar() or 0
                
                # ä»Šæ—¥æ–°å‘ç°
                today_result = await db.execute(
                    text("SELECT COUNT(*) FROM product_trends WHERE DATE(created_at) = CURRENT_DATE")
                )
                today_count = today_result.scalar() or 0
                
                # é«˜è¶‹åŠ¿äº§å“æ•°
                high_trend_result = await db.execute(
                    text("SELECT COUNT(*) FROM product_trends WHERE trend_score >= 70")
                )
                high_trend_count = high_trend_result.scalar() or 0
                
                # å·²å‘é€é‚®ä»¶æ•°
                emailed_result = await db.execute(
                    text("SELECT COUNT(*) FROM product_trends WHERE is_email_sent = true")
                )
                emailed_count = emailed_result.scalar() or 0
                
                # æŒ‰ç±»åˆ«ç»Ÿè®¡
                category_result = await db.execute(
                    text("""
                        SELECT category, COUNT(*) 
                        FROM product_trends 
                        GROUP BY category
                        ORDER BY COUNT(*) DESC
                        LIMIT 10
                    """)
                )
                by_category = {row[0]: row[1] for row in category_result.fetchall()}
                
                # æœ€æ–°å‘ç°çš„äº§å“
                recent_result = await db.execute(
                    text("""
                        SELECT product_name, category, trend_score, source_url, created_at
                        FROM product_trends
                        ORDER BY created_at DESC
                        LIMIT 5
                    """)
                )
                recent_products = [
                    {
                        "name": row[0],
                        "category": row[1],
                        "score": row[2],
                        "url": row[3],
                        "created_at": row[4].isoformat() if row[4] else None
                    }
                    for row in recent_result.fetchall()
                ]
                
                return {
                    "total": total,
                    "today": today_count,
                    "high_trend": high_trend_count,
                    "emailed": emailed_count,
                    "by_category": by_category,
                    "recent_products": recent_products
                }
                
        except Exception as e:
            self.log(f"è·å–äº§å“è¶‹åŠ¿ç»Ÿè®¡å¤±è´¥: {e}", "error")
            return {"error": str(e)}


# æ³¨å†ŒAgent
lead_hunter_agent = LeadHunterAgent()
AgentRegistry.register(lead_hunter_agent)
